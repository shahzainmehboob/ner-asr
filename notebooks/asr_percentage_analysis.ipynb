{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "import Levenshtein\n",
    "import string\n",
    "import difflib\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tag_values = ['O', 'PER', 'LOC', 'ORG']\n",
    "#tag_values = ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_whole_word_mask=True)\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"../model_2/bert_base_conll_50.pt\", map_location=torch.device('cpu'))) \n",
    "model.load_state_dict(torch.load(\"../model/with_punctuation_with_all_broken_entities_vertical_concatenation_100_2.pt\", map_location=torch.device('cpu'))) # WITH PHONETIC NOISE\n",
    "#model.load_state_dict(torch.load(\"../model_2/bert_base_conll_with_punctuation_with_broken_entities_75.pt\", map_location=torch.device('cpu'))) # WITH BROKEN ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_test(filepath):\n",
    "    df = pd.read_csv(filepath, sep=\";\")\n",
    "    #df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    #df = df[:6723]\n",
    "    g_test = df.groupby(\"Sentence #\")\n",
    "    test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    test_df.reset_index(inplace=True)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(data, tokenizer, model):\n",
    "    test = []\n",
    "    #results = open(\"conll03_base_ljspeech_asr_test_without_gpe_uncased_results_lower.txt\", \"a+\")\n",
    "    #test_data=original_data['sentence'].values.tolist()\n",
    "    #test_data=original_sentence\n",
    "    #test_data=test_df['Sentence'].values.tolist()\n",
    "    test_data=data\n",
    "\n",
    "    # ASR TEST DATE LATEST\n",
    "    sentence_no = 0\n",
    "    for data in test_data:\n",
    "        tokenized_sentence = tokenizer.encode(data.lower().strip())\n",
    "        #tokenized_sentence = nlp(data.lower().strip())\n",
    "        input_ids = torch.tensor([tokenized_sentence])\n",
    "        #input_ids = torch.tensor([tokenized_sentence._.trf_word_pieces])\n",
    "\n",
    "        with torch.no_grad():\n",
    "             output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        #tokens = _.trf_word_pieces_\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(tag_values[label_idx])\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            #result = str(sentence_no) + \"\\t\" + label + \"\\t\" + token + \"\\n\"\n",
    "            #results.write(result)\n",
    "            test.append((str(sentence_no), label, token))\n",
    "        sentence_no = sentence_no + 1\n",
    "    test_df = pd.DataFrame(test, columns=['sentence_no', 'labels', 'token'])\n",
    "    return test_df\n",
    "    #test_df.to_csv(\"final_asr_test_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_output(test_df, df):\n",
    "    indexNames = test_df[test_df['token'] == \"[CLS]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    indexNames = test_df[test_df['token'] == \"[SEP]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    test_df['label_asr'] = df['Tag']\n",
    "    test_df['token_asr'] = df['Word']\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(test_df, tags):\n",
    "    new_acc = accuracy_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_acc)\n",
    "\n",
    "    new_f1 = f1_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_f1)\n",
    "    print(\"---STATISTICS ON EACH LABEL---\")\n",
    "    for tag in tags:\n",
    "        true_positive = test_df[((test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_positive))\n",
    "        false_positive = test_df[((test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_positive))\n",
    "        false_negative = test_df[((~test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_negative))\n",
    "        true_negative = test_df[((~test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_negative))\n",
    "        prec = len(true_positive) / (len(true_positive) + len(false_positive))\n",
    "        print(prec)\n",
    "        recall = len(true_positive) / (len(true_positive) + len(false_negative))\n",
    "        print(recall)\n",
    "        f_measure = (2 * prec * recall) / (prec + recall)\n",
    "        print(f_measure)\n",
    "        print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = prepare_data_for_test('unprocessed_sampled_asr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>for although the Chinese took Impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands by a similar process</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>what the first Bible actually dated which also was printed at mace by Peter Shaffer in the year 1462</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,LOC,O,PER,PER,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>especially as regards to lowercase letters and type very similar was used during the next 15 or 20 years not only by chauffeur</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>Buy printers in Strasburg basil Paris Lubec and other cities</td>\n",
       "      <td>O,O,O,LOC,O,LOC,LOC,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>but don ' t expect in Italy letter with most often used</td>\n",
       "      <td>O,O,O,O,O,O,LOC,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #                                                                                                                                                    Sentence                                                Tag\n",
       "0         2.0  for although the Chinese took Impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands by a similar process  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O\n",
       "1        23.0                                                        what the first Bible actually dated which also was printed at mace by Peter Shaffer in the year 1462        O,O,O,O,O,O,O,O,O,O,O,LOC,O,PER,PER,O,O,O,O\n",
       "2        26.0                              especially as regards to lowercase letters and type very similar was used during the next 15 or 20 years not only by chauffeur    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER\n",
       "3        27.0                                                                                                Buy printers in Strasburg basil Paris Lubec and other cities                          O,O,O,LOC,O,LOC,LOC,O,O,O\n",
       "4        28.0                                                                                                     but don ' t expect in Italy letter with most often used                          O,O,O,O,O,O,LOC,O,O,O,O,O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(test_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>labels</th>\n",
       "      <th>token</th>\n",
       "      <th>label_asr</th>\n",
       "      <th>token_asr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8749</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>called</td>\n",
       "      <td>O</td>\n",
       "      <td>called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8750</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8751</th>\n",
       "      <td>472</td>\n",
       "      <td>ORG</td>\n",
       "      <td>cellar</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Cellar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8752</th>\n",
       "      <td>472</td>\n",
       "      <td>ORG</td>\n",
       "      <td>coffee</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8753</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_no labels   token label_asr token_asr\n",
       "8749         472      O  called         O    called\n",
       "8750         472      O     the         O       The\n",
       "8751         472    ORG  cellar       ORG    Cellar\n",
       "8752         472    ORG  coffee       ORG    Coffee\n",
       "8753         472      O   house         O     House"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'LOC', 'PER', 'ORG'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label_asr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_tag</th>\n",
       "      <th>asr_tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, PER, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              model_tag                                                                     asr_tag\n",
       "sentence_no                                                                                                                                                        \n",
       "0            [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]\n",
       "1                       [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]\n",
       "10                                 [O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]                        [O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]\n",
       "100                      [O, O, O, O, O, O, O, O, O, PER, O, O, O, O, O, O, O, O, O, O]                [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
       "101             [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_tag</th>\n",
       "      <th>asr_tag</th>\n",
       "      <th>asr_sentence_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, O, O, LOC, ORG, LOC, ORG, O, O, O]</td>\n",
       "      <td>[O, O, O, LOC, O, LOC, LOC, O, O, O]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, LOC, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, LOC, O, O, O, O, O]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    model_tag                                                                     asr_tag  asr_sentence_no\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]                0\n",
       "1             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]                1\n",
       "2       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]                2\n",
       "3                                      [O, O, O, LOC, ORG, LOC, ORG, O, O, O]                                        [O, O, O, LOC, O, LOC, LOC, O, O, O]                3\n",
       "4                                      [O, O, O, O, O, O, LOC, O, O, O, O, O]                                      [O, O, O, O, O, O, LOC, O, O, O, O, O]                4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9710989262051634\n",
      "F1 Score:  0.8881469115191986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_analysis(test_df, original_data_path):\n",
    "    g_asr = test_df.groupby(\"sentence_no\")\n",
    "    asr_df = pd.DataFrame({'Sentence': g_asr.apply(lambda sdf: \" \".join(map(str,sdf.token))),\n",
    "                      'Tag': g_asr.apply(lambda sdf: \",\".join(sdf.labels))})\n",
    "    asr_df['asr_sentence_no'] = asr_df.index\n",
    "    asr_df[[\"asr_sentence_no\"]] = asr_df[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "    asr_df.sort_values('asr_sentence_no', inplace=True)\n",
    "    asr_df.reset_index(drop=True, inplace=True)\n",
    "    original = pd.read_csv(original_data_path, sep=\";\")\n",
    "    #original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    #original = original[:6723]\n",
    "    g_original = original.groupby(\"Sentence #\")\n",
    "    original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    original_df.reset_index(inplace=True)\n",
    "    combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'].str.lower(),\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})\n",
    "    return asr_df, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_finding(tag, combined_df):\n",
    "#tag = \"PER\"\n",
    "    analysis = []\n",
    "    for i in range(0, len(combined_df), 1):\n",
    "        sample = combined_df.loc[[i]]\n",
    "        for original_sentence, asr_sentence, original_tag, asr_tag in zip(sample['original_sentence'].values.tolist(),\n",
    "                                                                          sample['asr_sentence'].values.tolist(),\n",
    "                                                                          sample['original_tags'].values.tolist(),\n",
    "                                                                          sample['asr_tags'].values.tolist()):\n",
    "            original_tag_token = np.array(original_tag.split(\",\"))\n",
    "            asr_tag_token = np.array(asr_tag.split(\",\"))\n",
    "            original_label = np.array(original_sentence.lower().split())\n",
    "            asr_label = np.array(asr_sentence.lower().split())\n",
    "\n",
    "            if tag in original_tag_token:\n",
    "                original_tag_ind = [index for index, element in enumerate(original_tag_token) if\n",
    "                                    original_tag_token[index] == tag]\n",
    "                if tag in asr_tag_token:\n",
    "                    asr_tag_ind = [index for index, element in enumerate(asr_tag_token) if\n",
    "                                       asr_tag_token[index] == tag]\n",
    "                    \n",
    "                    asr_tokens = []\n",
    "                    original_tokens = []\n",
    "                    errors = []\n",
    "                        # Sweynheim pannartz\n",
    "                        # Swain heim pannartz\n",
    "                    for ind in original_tag_ind:\n",
    "                        original_entity = original_label[ind]\n",
    "                        asr_entity = difflib.get_close_matches(original_entity, asr_label[asr_tag_ind])\n",
    "                        if len(asr_entity) > 0:\n",
    "                            asr_entity = asr_entity[0]\n",
    "                            error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(len(original_entity), len(asr_entity)))) * 100\n",
    "                            if error >= 50:\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(asr_label[ind])\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        else:\n",
    "                            asr_tokens.append(asr_label[ind])\n",
    "                            original_tokens.append(original_entity)\n",
    "                            errors.append(0.0)\n",
    "                    analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), True))\n",
    "                else:\n",
    "                    check = []\n",
    "                    o_label = original_label[original_tag_ind]\n",
    "                    for lab in o_label:\n",
    "                        j = 0\n",
    "                        for asr_lab in asr_label:\n",
    "                            local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                            if local_error >= 50.0:\n",
    "                                check.append(j)\n",
    "                            j = j + 1\n",
    "                    if len(check) > 0:\n",
    "                        asr_tokens = []\n",
    "                        original_tokens = []\n",
    "                        errors = []\n",
    "                        for ind in original_tag_ind:\n",
    "                            original_entity = original_label[ind]\n",
    "                            asr_entity = difflib.get_close_matches(original_entity, asr_label[check])\n",
    "                            if len(asr_entity) > 0:\n",
    "                                asr_entity = asr_entity[0]\n",
    "                                error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(\n",
    "                                len(original_entity), len(asr_entity)))) * 100\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(asr_label[ind])\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), False))\n",
    "                    else:\n",
    "                        analysis.append((i, original_label[original_tag_ind], [\"None\"], [0.0], 0.0, False))\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_asr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"ORG\", combined_df), columns=['Sample #', 'ASR Output', 'Model Output', 'Lavenstein', 'Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.42857142857143\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.285714285714285\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample #</th>\n",
       "      <th>ASR Output</th>\n",
       "      <th>Model Output</th>\n",
       "      <th>Lavenstein</th>\n",
       "      <th>Lavenstein Mean</th>\n",
       "      <th>Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>96</td>\n",
       "      <td>[kings, bench]</td>\n",
       "      <td>[kings, bench]</td>\n",
       "      <td>[100.0, 0.0]</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>97</td>\n",
       "      <td>[the, fleet]</td>\n",
       "      <td>[the, fleet]</td>\n",
       "      <td>[0.0, 100.0]</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>120</td>\n",
       "      <td>[kings, bench]</td>\n",
       "      <td>[kings, bench]</td>\n",
       "      <td>[100.0, 0.0]</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>125</td>\n",
       "      <td>[marshalsea, court]</td>\n",
       "      <td>[marshalsea, court]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>254</td>\n",
       "      <td>[prison, discipline, society]</td>\n",
       "      <td>[prison, discipline, society]</td>\n",
       "      <td>[0.0, 0.0, 100.0]</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sample #                     ASR Output                   Model Output         Lavenstein  Lavenstein Mean  Flag\n",
       "6         96                 [kings, bench]                 [kings, bench]       [100.0, 0.0]        50.000000  True\n",
       "7         97                   [the, fleet]                   [the, fleet]       [0.0, 100.0]        50.000000  True\n",
       "14       120                 [kings, bench]                 [kings, bench]       [100.0, 0.0]        50.000000  True\n",
       "17       125            [marshalsea, court]            [marshalsea, court]         [0.0, 0.0]         0.000000  True\n",
       "36       254  [prison, discipline, society]  [prison, discipline, society]  [0.0, 0.0, 100.0]        33.333333  True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.285714285714285\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.42857142857143, 14.285714285714285, 39.285714285714285, 0.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "# [84.375, 3.65, 11.97] PER\n",
    "# [162, 7, 23] PER count\n",
    "\n",
    "\n",
    "# [84.92, 8.33, 6.746] LOC\n",
    "# [214, 21, 17] LOC count\n",
    "\n",
    "\n",
    "# [40.476, 13.09, 46.42] ORG\n",
    "# [34, 11, 39] ORG count\n",
    "\n",
    "# [69.92, 8.35, 21.71] Total Percentage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [85.41666666666666, 5.208333333333334, 9.375, 0.0] PER broken\n",
    "# [164, 10, 18, 0] PER broken count\n",
    "\n",
    "\n",
    "# [83.73015873015873, 3.968253968253968, 12.3015873015873, 0.0] LOC broken\n",
    "# [211, 10, 31, 0] LOC broken count\n",
    "\n",
    "\n",
    "# [46.42857142857143, 14.285714285714285, 39.285714285714285, 0.0] ORG broken\n",
    "# [39, 12, 33, 0] ORG broken count\n",
    "\n",
    "\n",
    "# [71.85, 7.81, 20.31] total broken percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 12, 33, 0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [131, 8, 18, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
