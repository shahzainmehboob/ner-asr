{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "import Levenshtein\n",
    "import string\n",
    "import difflib\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values = ['O', 'PER', 'LOC', 'ORG']\n",
    "#tag_values = ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_whole_word_mask=True)\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False)\n",
    "model.load_state_dict(torch.load(\"../model/bert_base_conll_without_punctuation_lower_case_75.pt\", map_location=torch.device('cpu')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_test(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[:7851]\n",
    "    g_test = df.groupby(\"Sentence #\")\n",
    "    test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    test_df.reset_index(inplace=True)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(data, tokenizer, model):\n",
    "    test = []\n",
    "    #results = open(\"conll03_base_ljspeech_asr_test_without_gpe_uncased_results_lower.txt\", \"a+\")\n",
    "    #test_data=original_data['sentence'].values.tolist()\n",
    "    #test_data=original_sentence\n",
    "    #test_data=test_df['Sentence'].values.tolist()\n",
    "    test_data=data\n",
    "\n",
    "    # ASR TEST DATE LATEST\n",
    "    sentence_no = 0\n",
    "    for data in test_data:\n",
    "        tokenized_sentence = tokenizer.encode(data.lower().strip())\n",
    "        #tokenized_sentence = nlp(data.lower().strip())\n",
    "        input_ids = torch.tensor([tokenized_sentence])\n",
    "        #input_ids = torch.tensor([tokenized_sentence._.trf_word_pieces])\n",
    "\n",
    "        with torch.no_grad():\n",
    "             output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        #tokens = _.trf_word_pieces_\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(tag_values[label_idx])\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            #result = str(sentence_no) + \"\\t\" + label + \"\\t\" + token + \"\\n\"\n",
    "            #results.write(result)\n",
    "            test.append((str(sentence_no), label, token))\n",
    "        sentence_no = sentence_no + 1\n",
    "    test_df = pd.DataFrame(test, columns=['sentence_no', 'labels', 'token'])\n",
    "    return test_df\n",
    "    #test_df.to_csv(\"final_asr_test_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_output(test_df, df):\n",
    "    indexNames = test_df[test_df['token'] == \"[CLS]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    indexNames = test_df[test_df['token'] == \"[SEP]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    test_df['label_asr'] = df['Tag']\n",
    "    test_df['token_asr'] = df['Word']\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(test_df, tags):\n",
    "    new_acc = accuracy_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_acc)\n",
    "\n",
    "    new_f1 = f1_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_f1)\n",
    "    print(\"---STATISTICS ON EACH LABEL---\")\n",
    "    for tag in tags:\n",
    "        true_positive = test_df[((test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_positive))\n",
    "        false_positive = test_df[((test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_positive))\n",
    "        false_negative = test_df[((~test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_negative))\n",
    "        true_negative = test_df[((~test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_negative))\n",
    "        prec = len(true_positive) / (len(true_positive) + len(false_positive))\n",
    "        print(prec)\n",
    "        recall = len(true_positive) / (len(true_positive) + len(false_negative))\n",
    "        print(recall)\n",
    "        f_measure = (2 * prec * recall) / (prec + recall)\n",
    "        print(f_measure)\n",
    "        print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = prepare_data_for_test('unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>For</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>although</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>took</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #      Word Tag\n",
       "0           2       For   O\n",
       "1           2  although   O\n",
       "2           2       the   O\n",
       "3           2   Chinese   O\n",
       "4           2      took   O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands , by a similar process</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>But the first Bible actually dated ( which also was printed at Maintz by Peter Schoeffer in the year 1462 )</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,PER,PER,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>especially as regards the lower - case letters and type very similar was used during the next fifteen or twenty years not only by Schoeffer ,</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>but by printers in Strasburg , Basle , Paris , Lubeck , and other cities .</td>\n",
       "      <td>O,O,O,O,LOC,O,LOC,O,LOC,O,LOC,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>But though on the whole , except in Italy , Gothic letter was most often used</td>\n",
       "      <td>O,O,O,O,O,O,O,O,LOC,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #                                                                                                                                                      Sentence                                                    Tag\n",
       "0           2  For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands , by a similar process    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O,O\n",
       "1          23                                                   But the first Bible actually dated ( which also was printed at Maintz by Peter Schoeffer in the year 1462 )        O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,PER,PER,O,O,O,O,O\n",
       "2          26                 especially as regards the lower - case letters and type very similar was used during the next fifteen or twenty years not only by Schoeffer ,  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER,O\n",
       "3          27                                                                                    but by printers in Strasburg , Basle , Paris , Lubeck , and other cities .                O,O,O,O,LOC,O,LOC,O,LOC,O,LOC,O,O,O,O,O\n",
       "4          28                                                                                 But though on the whole , except in Italy , Gothic letter was most often used                      O,O,O,O,O,O,O,O,LOC,O,O,O,O,O,O,O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # general\n",
    "    phrase = re.sub(r\" \\' s\", \"\\'s\", phrase)\n",
    "    phrase = re.sub(r\" \\' t\", \"\\'t\", phrase)\n",
    "    phrase = re.sub(r\" n \\' t\", \"n't\", phrase)\n",
    "    phrase = re.sub(r\" \\' re\", \"\\'re\", phrase)\n",
    "    phrase = re.sub(r\" \\' d\", \"\\'d\", phrase)\n",
    "    phrase = re.sub(r\" \\' ll\", \"\\'ll\", phrase)\n",
    "    phrase = re.sub(r\" \\' t\", \"\\'t\", phrase)\n",
    "    phrase = re.sub(r\" \\' ve\", \"\\'ve\", phrase)\n",
    "    phrase = re.sub(r\" \\' m\", \"\\'m\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_data = []\n",
    "sentence_no = 0\n",
    "new_df = []\n",
    "for sentence, tags in zip(test_df['Sentence'].values.tolist(), test_df['Tag'].values.tolist()):\n",
    "    words = sentence.split(\" \")\n",
    "    ind = [ i for i, word in enumerate(words) if word == \"'s\" or word == \"'t\" or word == \"'re\" or word == \"'d\" or word == \"'ll\" or word == \"'t\" or word == \"'ve\" or word == \"'m\" or word == \"n't\"]\n",
    "    labels = tags.split(\",\")\n",
    "    sentence = \" \".join(words)\n",
    "    sentence = decontracted(sentence)\n",
    "    tags = [ elem for i, elem in enumerate(labels) if i not in ind]\n",
    "    stripped = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    stripped = stripped.split(\" \")\n",
    "    for word, label in zip(stripped, tags):\n",
    "        p_data.append((sentence_no, word, label))\n",
    "    sentence_no = sentence_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(p_data, columns=['Sentence #', 'Word', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  20,   31,   45,   51,   71,   77,   79,   81,   83,   87,\n",
       "            ...\n",
       "            7698, 7701, 7708, 7714, 7724, 7727, 7730, 7739, 7746, 7756], dtype='int64', length=1031)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['Word'] == \"\"].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(test[test['Word'] == \"\"].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>7751</td>\n",
       "      <td>375</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>7752</td>\n",
       "      <td>375</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723</th>\n",
       "      <td>7753</td>\n",
       "      <td>375</td>\n",
       "      <td>governors</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>7754</td>\n",
       "      <td>375</td>\n",
       "      <td>own</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6725</th>\n",
       "      <td>7755</td>\n",
       "      <td>375</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Sentence #       Word Tag\n",
       "6721   7751         375         in   O\n",
       "6722   7752         375        the   O\n",
       "6723   7753         375  governors   O\n",
       "6724   7754         375        own   O\n",
       "6725   7755         375      house   O"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test.groupby(\"Sentence #\")\n",
    "test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>371</td>\n",
       "      <td>On this female side where the Ladies Association still reigned supreme more system and a greater semblance of decorum was maintained</td>\n",
       "      <td>O,O,O,O,O,O,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>372</td>\n",
       "      <td>The separation of the sexes was not indeed rigidly carried out in Newgate as yet</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>373</td>\n",
       "      <td>and the men could also at any time go for tea coffee and sugar to Mrs Browns shop which was inside the female gate</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>374</td>\n",
       "      <td>Some member of the Ladies Association observed and commented upon the fact that a young rosy cheeked girl had been kept by the governor from transportation</td>\n",
       "      <td>O,O,O,O,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>375</td>\n",
       "      <td>committed by the House of Commons who had been lodged in the governors own house</td>\n",
       "      <td>O,O,O,ORG,ORG,ORG,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #                                                                                                                                                     Sentence                                                      Tag\n",
       "371         371                         On this female side where the Ladies Association still reigned supreme more system and a greater semblance of decorum was maintained            O,O,O,O,O,O,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O,O,O\n",
       "372         372                                                                             The separation of the sexes was not indeed rigidly carried out in Newgate as yet                          O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O\n",
       "373         373                                           and the men could also at any time go for tea coffee and sugar to Mrs Browns shop which was inside the female gate        O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER,O,O,O,O,O,O,O\n",
       "374         374  Some member of the Ladies Association observed and commented upon the fact that a young rosy cheeked girl had been kept by the governor from transportation  O,O,O,O,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O\n",
       "375         375                                                                             committed by the House of Commons who had been lodged in the governors own house                      O,O,O,ORG,ORG,ORG,O,O,O,O,O,O,O,O,O"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(test_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>labels</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence_no, labels, token]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['token'] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>labels</th>\n",
       "      <th>token</th>\n",
       "      <th>label_asr</th>\n",
       "      <th>token_asr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>governors</td>\n",
       "      <td>O</td>\n",
       "      <td>governors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>own</td>\n",
       "      <td>O</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6725</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_no labels      token label_asr  token_asr\n",
       "6721         375      O         in         O         in\n",
       "6722         375      O        the         O        the\n",
       "6723         375      O  governors         O  governors\n",
       "6724         375      O        own         O        own\n",
       "6725         375      O      house         O      house"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'LOC', 'PER', 'ORG'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label_asr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_tag</th>\n",
       "      <th>asr_tag</th>\n",
       "      <th>asr_sentence_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, O, O, O, LOC, LOC, LOC, LOC, O, O, O]</td>\n",
       "      <td>[O, O, O, O, LOC, LOC, LOC, LOC, O, O, O]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, O, LOC, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, LOC, O, O, O, O, O, O]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    model_tag                                                                     asr_tag  asr_sentence_no\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]                0\n",
       "1             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]                1\n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]                2\n",
       "3                                   [O, O, O, O, LOC, LOC, LOC, LOC, O, O, O]                                   [O, O, O, O, LOC, LOC, LOC, LOC, O, O, O]                3\n",
       "4                                [O, O, O, O, O, O, O, LOC, O, O, O, O, O, O]                                [O, O, O, O, O, O, O, LOC, O, O, O, O, O, O]                4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9588165328575676\n",
      "F1 Score:  0.8503144654088051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_analysis(test_df, original_data_path):\n",
    "    g_asr = test_df.groupby(\"sentence_no\")\n",
    "    asr_df = pd.DataFrame({'Sentence': g_asr.apply(lambda sdf: \" \".join(map(str,sdf.token))),\n",
    "                      'Tag': g_asr.apply(lambda sdf: \",\".join(sdf.labels))})\n",
    "    asr_df['asr_sentence_no'] = asr_df.index\n",
    "    asr_df[[\"asr_sentence_no\"]] = asr_df[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "    asr_df.sort_values('asr_sentence_no', inplace=True)\n",
    "    asr_df.reset_index(drop=True, inplace=True)\n",
    "    original = pd.read_csv(original_data_path)\n",
    "    original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    original = original[:7851]\n",
    "    g_original = original.groupby(\"Sentence #\")\n",
    "    original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    original_df.reset_index(inplace=True)\n",
    "    combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'].str.lower(),\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})\n",
    "    return asr_df, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_finding(tag, combined_df):\n",
    "#tag = \"PER\"\n",
    "    analysis = []\n",
    "    for i in range(0, len(combined_df), 1):\n",
    "        sample = combined_df.loc[[i]]\n",
    "        for original_sentence, asr_sentence, original_tag, asr_tag in zip(sample['original_sentence'].values.tolist(),\n",
    "                                                                          sample['asr_sentence'].values.tolist(),\n",
    "                                                                          sample['original_tags'].values.tolist(),\n",
    "                                                                          sample['asr_tags'].values.tolist()):\n",
    "            original_tag_token = np.array(original_tag.split(\",\"))\n",
    "            asr_tag_token = np.array(asr_tag.split(\",\"))\n",
    "            original_label = np.array(original_sentence.lower().split())\n",
    "            asr_label = np.array(asr_sentence.lower().split())\n",
    "\n",
    "            if tag in original_tag_token:\n",
    "                original_tag_ind = [index for index, element in enumerate(original_tag_token) if\n",
    "                                    original_tag_token[index] == tag]\n",
    "                if tag in asr_tag_token:\n",
    "                    asr_tag_ind = [index for index, element in enumerate(asr_tag_token) if\n",
    "                                       asr_tag_token[index] == tag]\n",
    "                    \n",
    "                    asr_tokens = []\n",
    "                    original_tokens = []\n",
    "                    errors = []\n",
    "                        # Sweynheim pannartz\n",
    "                        # Swain heim pannartz\n",
    "                    for ind in original_tag_ind:\n",
    "                        original_entity = original_label[ind]\n",
    "                        asr_entity = difflib.get_close_matches(original_entity, asr_label[asr_tag_ind])\n",
    "                        if len(asr_entity) > 0:\n",
    "                            asr_entity = asr_entity[0]\n",
    "                            error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(len(original_entity), len(asr_entity)))) * 100\n",
    "                            if error >= 50:\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(\"None\")\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        else:\n",
    "                            asr_tokens.append(\"None\")\n",
    "                            original_tokens.append(original_entity)\n",
    "                            errors.append(0.0)\n",
    "                    analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), True))\n",
    "                else:\n",
    "                    check = []\n",
    "                    o_label = original_label[original_tag_ind]\n",
    "                    for lab in o_label:\n",
    "                        j = 0\n",
    "                        for asr_lab in asr_label:\n",
    "                            local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                            if local_error >= 50.0:\n",
    "                                check.append(j)\n",
    "                            j = j + 1\n",
    "                    if len(check) > 0:\n",
    "                        asr_tokens = []\n",
    "                        original_tokens = []\n",
    "                        errors = []\n",
    "                        for ind in original_tag_ind:\n",
    "                            original_entity = original_label[ind]\n",
    "                            asr_entity = difflib.get_close_matches(original_entity, asr_label[check])\n",
    "                            if len(asr_entity) > 0:\n",
    "                                asr_entity = asr_entity[0]\n",
    "                                error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(\n",
    "                                len(original_entity), len(asr_entity)))) * 100\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(\"None\")\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), False))\n",
    "                    else:\n",
    "                        analysis.append((i, original_label[original_tag_ind], [\"None\"], [0.0], 0.0, False))\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein', 'Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([100.0,100.0]) == 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_nofound.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_analysis(sample_df, combined_df):\n",
    "    ind = np.array(sample_df['Sample #'].values.tolist())\n",
    "    df = combined_df.loc[ind]\n",
    "    df.insert(2,'Original',sample_df['Original'].values.tolist())\n",
    "    df.insert(5,'ASR',sample_df['ASR'].values.tolist())\n",
    "    df.drop(['original_tags', 'asr_tags'], axis=1, inplace=True)\n",
    "    df.head(50)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern = pattern_analysis(orig_asr_similar, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling(df):\n",
    "    i = 0\n",
    "    equal_length_samples = []\n",
    "    variable_length_samples = []\n",
    "    for sample, original, asr in zip(df.index, \n",
    "                                     df['Original'],\n",
    "                                     df['ASR']):\n",
    "        if len(original) == len(asr):\n",
    "            equal_length_samples.append(sample)\n",
    "        else:\n",
    "            variable_length_samples.append(sample)\n",
    "    equal_length_samples.sort()\n",
    "    variable_length_samples.sort()\n",
    "    equal_length_samples_df = df.loc[equal_length_samples]\n",
    "    variable_length_samples_df = df.loc[variable_length_samples]\n",
    "    return equal_length_samples_df, variable_length_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df, variable_length_words_samples_df = error_sampling(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variable_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variable_length_words_samples_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_words_simulation(sampled_df):\n",
    "    simulated_asr = []\n",
    "    for sample, original_sentence, asr_sentence, original, asr in zip(sampled_df.index,\n",
    "                                     sampled_df['original_sentence'],\n",
    "                                     sampled_df['asr_sentence'],\n",
    "                                     sampled_df['Original'],\n",
    "                                     sampled_df['ASR']):\n",
    "\n",
    "        for x,y in zip(original, asr):\n",
    "            #original_words.append(x)\n",
    "            #asr_words.append(y)\n",
    "            if y in asr_sentence:\n",
    "                asr_sentence = asr_sentence.replace(y, x)\n",
    "            \n",
    "        simulated_asr.append((sample, asr_sentence))\n",
    "    simulated_asr_df = pd.DataFrame(simulated_asr)\n",
    "    return simulated_asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_words_simulation(df):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "            df.index,\n",
    "            df['original_sentence'].values.tolist(),\n",
    "            df['asr_sentence'].values.tolist(),\n",
    "            df['Original'].values.tolist(),\n",
    "            df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        o_label = original_label[original_tag_ind]\n",
    "        for lab in original_tag:\n",
    "            for asr_lab in asr_tag:\n",
    "                local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                if local_error >= 50.0:\n",
    "                    asr_sentence = asr_sentence.replace(asr_lab, lab)\n",
    "        check.append((sample, asr_sentence))\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df(asr_df, simulated_df):\n",
    "    asr_df.loc[simulated_df[0].values.tolist(), 'Sentence'] = simulated_df[1].values.tolist()\n",
    "    return asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = equal_words_simulation(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulated_asr_df = variable_words_simulation(variable_length_words_samples_df)\n",
    "#simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = prepare_model_output(test_df, new_df)\n",
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_similar.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_simulated_df = combined_df.loc[orig_asr_similar['Sample #'].values.tolist(),['original_sentence','original_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_simulated_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = df.groupby(\"Sentence #\")\n",
    "x = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no = list(range(0, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.index = sentence_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[context_simulated_df.index.tolist(), 'Sentence'] = context_simulated_df['original_sentence'].values.tolist()\n",
    "x.loc[context_simulated_df.index.tolist(), 'Tag'] = context_simulated_df['original_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_simulated_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.loc[context_simulated_df.index.tolist(), 'Sentence'] = context_simulated_df['original_sentence'].values.tolist()\n",
    "asr_df.loc[context_simulated_df.index.tolist(), 'Tag'] = context_simulated_df['original_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no = 0\n",
    "dataset=[]\n",
    "for sentences, tags in zip(x['Sentence'].values.tolist(), x['Tag'].values.tolist()):\n",
    "    sentence=sentences.split(\" \")\n",
    "    tag = tags.split(\",\")\n",
    "    for word, label in zip(sentence, tag):\n",
    "        dataset.append((sentence_no, word, label))\n",
    "    sentence_no = sentence_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(dataset, columns=['Sentence #', 'Word', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"ORG\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_context(df, n_grams):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "                df.index,\n",
    "                df['original_sentence'].values.tolist(),\n",
    "                df['asr_sentence'].values.tolist(),\n",
    "                df['Original'].values.tolist(),\n",
    "                df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        for l in original_tag_ind:\n",
    "            if l <= (len(original_label)-1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams+1, 1):\n",
    "                    if l+c >= 0:\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l+c < len(original_label):\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "        for l in asr_tag_ind:\n",
    "            if l <= (len(asr_label) - 1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams + 1, 1):\n",
    "                    if l + c >= 0:\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l + c < len(asr_label):\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "        check.append((sample, (\" | \").join(original_bigrams), original_sentence, original_tag, (\" | \").join(asr_bigrams), asr_sentence, asr_tag))\n",
    "    context = pd.DataFrame(check)\n",
    "    context.columns = ['Sample #', 'Original N-Grams', \"original_sentence\", \"Original\", \"ASR N-Grams\", \"asr_sentence\", \"ASR\"]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling3(context):\n",
    "    check = []\n",
    "    for sample, original_ngrams, original_sentence, asr_ngrams, asr_sentence, original_tag, asr_tag in zip(\n",
    "            context['Sample #'].values.tolist(),\n",
    "            context['Original N-Grams'].values.tolist(),\n",
    "            context['original_sentence'].values.tolist(),\n",
    "            context['ASR N-Grams'].values.tolist(),\n",
    "            context['asr_sentence'].values.tolist(),\n",
    "            context['Original'].values.tolist(),\n",
    "            context['ASR'].values.tolist()):\n",
    "\n",
    "        original_ngrams = np.array(original_ngrams.split(\"|\"))\n",
    "        asr_ngrams = np.array(asr_ngrams.split(\"|\"))\n",
    "\n",
    "        local_errors = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for _original in original_tag:\n",
    "            if _original in asr_tag:\n",
    "                if len(asr_ngrams) < len(asr_tag):\n",
    "                    continue\n",
    "                \n",
    "                print(asr_sentence)\n",
    "                asr_sentence = asr_sentence.replace(\"\".join(asr_ngrams[i].rstrip()), \"\".join(original_ngrams[i].rstrip()))\n",
    "                print(asr_sentence)\n",
    "                #print(check)\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "            else:\n",
    "                j = j + 1\n",
    "        check.append((sample, asr_sentence))\n",
    "        print(\"---------------\")\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = finding_context(equal_length_words_samples_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = error_sampling3(context)\n",
    "simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = finding_context(variable_length_words_samples_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = error_sampling3(context)\n",
    "simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"and though the famous family of aldus restored its technical excellence , rejecting battered letters ,\",\"most of caxton ' s zone types of an earlier character\", \"are the leaders in this luckless change , though our own baskerville , who was at work some years before them , went much on the same lines\",\n",
    "       \"now come into general use that are obviously a great improvement on the ordinary \\\" modern style \\\" and use in england , which is in fact the bodoni type\" , \"on the top of the jail , continues neild , arawatch - house and a century - box , where two or more guards , with dogs and firearms ,\" ,\n",
    "       \"these courts were extended to centuries later to several large provincial towns , and all were in full activity when neild road ,\" , \"he had been in the employ of a corn - chandler at islington , and went into london with his master ' s cart and horse .\",\n",
    "       \"shameful malpractices of bambridge ,\" , \"if they happened to be in funds - - among whom was the marquis of slego in 1811\", \n",
    "       \"mister . neild , a second howard ,\", \"again the 22 charles ii . c20 order the jailer to keep felons and debtors \\\" separate and apart from one another ,\",\n",
    "       \"prisoners were crowded together in the jail , contrary to the requirements of the for george the 4th .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"have now come into general use and are obviously a great improvement on the ordinary \\\" modern style \\\" in use in england , which is in fact the bodoni type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = model_test([test], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
