{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "import Levenshtein\n",
    "import string\n",
    "import difflib\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_values = ['O', 'PER', 'LOC', 'ORG']\n",
    "#tag_values = ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_whole_word_mask=True)\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False)\n",
    "model.load_state_dict(torch.load(\"../model/bert_base_conll_lower_case_100.pt\", map_location=torch.device('cpu')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_test(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[:6723]\n",
    "    g_test = df.groupby(\"Sentence #\")\n",
    "    test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    test_df.reset_index(inplace=True)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(data, tokenizer, model):\n",
    "    test = []\n",
    "    #results = open(\"conll03_base_ljspeech_asr_test_without_gpe_uncased_results_lower.txt\", \"a+\")\n",
    "    #test_data=original_data['sentence'].values.tolist()\n",
    "    #test_data=original_sentence\n",
    "    #test_data=test_df['Sentence'].values.tolist()\n",
    "    test_data=data\n",
    "\n",
    "    # ASR TEST DATE LATEST\n",
    "    sentence_no = 0\n",
    "    for data in test_data:\n",
    "        tokenized_sentence = tokenizer.encode(data.lower().strip())\n",
    "        #tokenized_sentence = nlp(data.lower().strip())\n",
    "        input_ids = torch.tensor([tokenized_sentence])\n",
    "        #input_ids = torch.tensor([tokenized_sentence._.trf_word_pieces])\n",
    "\n",
    "        with torch.no_grad():\n",
    "             output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        #tokens = _.trf_word_pieces_\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(tag_values[label_idx])\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            #result = str(sentence_no) + \"\\t\" + label + \"\\t\" + token + \"\\n\"\n",
    "            #results.write(result)\n",
    "            test.append((str(sentence_no), label, token))\n",
    "        sentence_no = sentence_no + 1\n",
    "    test_df = pd.DataFrame(test, columns=['sentence_no', 'labels', 'token'])\n",
    "    return test_df\n",
    "    #test_df.to_csv(\"final_asr_test_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_output(test_df, df):\n",
    "    indexNames = test_df[test_df['token'] == \"[CLS]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    indexNames = test_df[test_df['token'] == \"[SEP]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    test_df['label_asr'] = df['Tag']\n",
    "    test_df['token_asr'] = df['Word']\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(test_df, tags):\n",
    "    new_acc = accuracy_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_acc)\n",
    "\n",
    "    new_f1 = f1_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_f1)\n",
    "    print(\"---STATISTICS ON EACH LABEL---\")\n",
    "    for tag in tags:\n",
    "        true_positive = test_df[((test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_positive))\n",
    "        false_positive = test_df[((test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_positive))\n",
    "        false_negative = test_df[((~test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_negative))\n",
    "        true_negative = test_df[((~test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_negative))\n",
    "        prec = len(true_positive) / (len(true_positive) + len(false_positive))\n",
    "        print(prec)\n",
    "        recall = len(true_positive) / (len(true_positive) + len(false_negative))\n",
    "        print(recall)\n",
    "        f_measure = (2 * prec * recall) / (prec + recall)\n",
    "        print(f_measure)\n",
    "        print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = prepare_data_for_test('unprocessed_sampled_asr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(test_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label_asr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_analysis(test_df, original_data_path):\n",
    "    g_asr = test_df.groupby(\"sentence_no\")\n",
    "    asr_df = pd.DataFrame({'Sentence': g_asr.apply(lambda sdf: \" \".join(map(str,sdf.token))),\n",
    "                      'Tag': g_asr.apply(lambda sdf: \",\".join(sdf.labels))})\n",
    "    asr_df['asr_sentence_no'] = asr_df.index\n",
    "    asr_df[[\"asr_sentence_no\"]] = asr_df[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "    asr_df.sort_values('asr_sentence_no', inplace=True)\n",
    "    asr_df.reset_index(drop=True, inplace=True)\n",
    "    original = pd.read_csv(original_data_path)\n",
    "    original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    original = original[:7851]\n",
    "    g_original = original.groupby(\"Sentence #\")\n",
    "    original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    original_df.reset_index(inplace=True)\n",
    "    combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'].str.lower(),\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})\n",
    "    return asr_df, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_finding(tag, combined_df):\n",
    "#tag = \"PER\"\n",
    "    analysis = []\n",
    "    for i in range(0, len(combined_df), 1):\n",
    "        sample = combined_df.loc[[i]]\n",
    "        for original_sentence, asr_sentence, original_tag, asr_tag in zip(sample['original_sentence'].values.tolist(),\n",
    "                                                                          sample['asr_sentence'].values.tolist(),\n",
    "                                                                          sample['original_tags'].values.tolist(),\n",
    "                                                                          sample['asr_tags'].values.tolist()):\n",
    "            original_tag_token = np.array(original_tag.split(\",\"))\n",
    "            asr_tag_token = np.array(asr_tag.split(\",\"))\n",
    "            original_label = np.array(original_sentence.lower().split())\n",
    "            asr_label = np.array(asr_sentence.lower().split())\n",
    "\n",
    "            if tag in original_tag_token:\n",
    "                original_tag_ind = [index for index, element in enumerate(original_tag_token) if\n",
    "                                    original_tag_token[index] == tag]\n",
    "                if tag in asr_tag_token:\n",
    "                    asr_tag_ind = [index for index, element in enumerate(asr_tag_token) if\n",
    "                                       asr_tag_token[index] == tag]\n",
    "                    \n",
    "                    asr_tokens = []\n",
    "                    original_tokens = []\n",
    "                    errors = []\n",
    "                        # Sweynheim pannartz\n",
    "                        # Swain heim pannartz\n",
    "                    for ind in original_tag_ind:\n",
    "                        original_entity = original_label[ind]\n",
    "                        asr_entity = difflib.get_close_matches(original_entity, asr_label[asr_tag_ind])\n",
    "                        if len(asr_entity) > 0:\n",
    "                            asr_entity = asr_entity[0]\n",
    "                            error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(len(original_entity), len(asr_entity)))) * 100\n",
    "                            if error >= 50:\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(\"None\")\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        else:\n",
    "                            asr_tokens.append(\"None\")\n",
    "                            original_tokens.append(original_entity)\n",
    "                            errors.append(0.0)\n",
    "                    analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), True))\n",
    "                else:\n",
    "                    check = []\n",
    "                    o_label = original_label[original_tag_ind]\n",
    "                    for lab in o_label:\n",
    "                        j = 0\n",
    "                        for asr_lab in asr_label:\n",
    "                            local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                            if local_error >= 50.0:\n",
    "                                check.append(j)\n",
    "                            j = j + 1\n",
    "                    if len(check) > 0:\n",
    "                        asr_tokens = []\n",
    "                        original_tokens = []\n",
    "                        errors = []\n",
    "                        for ind in original_tag_ind:\n",
    "                            original_entity = original_label[ind]\n",
    "                            asr_entity = difflib.get_close_matches(original_entity, asr_label[check])\n",
    "                            if len(asr_entity) > 0:\n",
    "                                asr_entity = asr_entity[0]\n",
    "                                error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(\n",
    "                                len(original_entity), len(asr_entity)))) * 100\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(\"None\")\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), False))\n",
    "                    else:\n",
    "                        analysis.append((i, original_label[original_tag_ind], [\"None\"], [0.0], 0.0, False))\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('unprocessed_sampled_original.csv')\n",
    "original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "original = original[:7851]\n",
    "g_original = original.groupby(\"Sentence #\")\n",
    "original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "original_df.reset_index(inplace=True)\n",
    "combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'],\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein', 'Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([100.0,100.0]) == 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_nofound.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"the position of our society that don ' t work of utility might be also a work of art , if we care to make it so .\",\n",
    " \"full details of the arrangements are to be found in mr . neil ' s ' state of prisons in england , scotland , and wales , ' published in 1812 .\",\n",
    " 'alfred the great established the court baron , the hundred court , and the county court , which among other matters entertain please for debt .',\n",
    " \"lake county court was the sheriff ' s , who said they ' re surrounded by the bishop and the magnets of the county\",\n",
    " 'so much inconvenience ensued , that in 1518 the corporation obtained from parliament and act empowering to alderman',\n",
    " 'four common councilman to hold courts of requests , or courts of conscience , to hear and determine all causes of death',\n",
    " 'mr . buxton , in his \" inquiry into the system of prison discipline , \"',\n",
    " 'the fleet , and the marshalsea prison especially devoted to them ,',\n",
    " 'whilst ludgate , the gilts 1st street , and borrow comforters also received them',\n",
    " 'the sale of spirits was forbidden , but june could always be had at the whistling shops , where it was known as moonshine , sky blue ,',\n",
    " 'the fleet , which stood in farrington street ,',\n",
    " 'the warden of the fleet at the commencement of the 18th century , are too well known to need more than a passing reference .',\n",
    " 'and came under the strong animated version of the jail committee of 1729 .',\n",
    " 'the lord steward of the household , the stewart and officers of the marshalsea court , and others .',\n",
    " 'comforters of ludgate , giltspur street , and the borough where discontinued as debtors \\' prisons ( as was newgate also )',\n",
    " \"clergyman , proctor ' s , attorneys , and persons specially selected by the corporation .\",\n",
    " 'at one time the ludgate debtors , accompanied by the keeper ,',\n",
    " \"spruce street compton received sheriff ' s debtors , also felons , vagrants , and knight charges .\",\n",
    " 'it was generally crowded , as debtors who would have gone to the poultry copter we sent to giltspur street when the former was condemned as unfit to receive prisoners .',\n",
    " \"the borough compter was in a disgraceful state to the last . the men ' s ward had an earth , or rather a mud , floor ,\",\n",
    " \"notably as when numbers filled new gate in anticipation of lord reds dale ' s bill for insolvent debtors ,\",\n",
    " 'is gradually was forced upon the consciousness of the corporation ,',\n",
    " 'bypass now to the criminal side of newgate , which consisted of the six quarters or yards already enumerated and describe .',\n",
    " 'court of aldermen appointed a committee of its own body , assisted by the town clock , mr . , city surveyor , sun to the architect ,',\n",
    " \"send mr . addison , keeper of new gate , to make a visitation of the jail ' s supposed to be the best managed , including those of petworth and gloucester .\",\n",
    " 'the committee did not deny the superior advantages offered by such prisons as gloucester and petworth ,',\n",
    " 'the committee does not seem to have yet understood that new gate could be only and properly replaced',\n",
    " 'buy a new jail built on the outskirts , as holloway eventually was , and committed itself to be altogether counter',\n",
    " \"i ' m checked in its efforts towards reform by the prohibitory costliness of the land about nougat .\",\n",
    " 'why not relieving you gate more largely upon the superior accommodation which build bank offer ?',\n",
    " 'chronicles of new gate , volume 2 . by arthur griffith . section 7 : the beginnings of prison reform .',\n",
    " 'i have shown in a previous chapter what new gate was at this , despite a vast expenditure and boasted efforts to introduce reforms .',\n",
    " 'one of the moving spirits was the honorable h . g . bennett , auntie , whose vigorous protests against the lamentable condition of newgate have already been recorded .',\n",
    " 'the chronicles of nougat , volume 2 . by arthur griffith . section 8 : the beginnings of prison reform .',\n",
    " 'newgate prisoners were the victims to another most objectionable practice which obtained all over london .',\n",
    " 'an imputation which the society indignantly and very justly repudiated , the statement being , as they said ,',\n",
    " 'among those from the society found a raid against it was sidney smith ,',\n",
    " 'admitting the good intentions of the society , he condemned there ultra humanitarianism as misplaced .',\n",
    " 'he took exception to various of the proposals of the society . he thought they linked too much to a system of indulgences and education in jails .',\n",
    " \"society pursuit it ' s laudable undertaking with a remarkable energy and great singleness of purpose .\",\n",
    " 'another point to which the society devoted infinite was the preparation of plans for the guidance of architects in the construction of prisons .',\n",
    " 'a very valuable volume published by the society',\n",
    " 'was introduced as early as 1790 by mr . blackburn',\n",
    " 'the society did not limit its remarks to the description of what had already been done',\n",
    " 'the prison society reproves the misdirected efforts of ambitious architect , to buy a lavish an improvident expenditure of public money',\n",
    " \"these are principles fully recognized now - a - days , and it may fairly be conceded that the prison discipline society ' s ideal\",\n",
    " 'after a few years of active exertion the society was rewarded by fresh legislation .',\n",
    " 'to its efforts , and their effect upon parliament and the public mind , we must attribute the new jail acts of for george the 4th',\n",
    " 'the promulgation of these to jail acts strengthen the hands of the prison discipline society enormously .',\n",
    " 'the society did not shrink from its self - imposed duty , but continued year after year , with unflagging energy and unflinching spirit , to watch closely',\n",
    " 'upon these and the private visitations made by various members of the society obtained effects ,',\n",
    " 'four years later the prison society reported',\n",
    " 'i just chillin by the report of the commissioners to inquire into the state of the municipal corporations in 1835 .',\n",
    " 'kidderminster had a prison one dance chill room ,',\n",
    " 'in 1827 the society was compelled to report that \" no material change has taken place in newgate since the passing of the prison laws ,',\n",
    " 'the prison society did not relax its efforts as time passed , but its leading members had other and more pressing claims upon their energies .',\n",
    " 'this committee anniversary strongly upon the system in force at the metropolitan jails , and more especially upon the condition of nougat',\n",
    " 'mister . samuel hoare was examined by this committee',\n",
    " 'i stated that in his opinion new gate , as the common jail of middlesex , was wholly inadequate to the proper confinement of its prisoners .',\n",
    " 'the committee was appointed , under the presidency of the duke of richmond',\n",
    " 'the whole question was again dealt with in lord john russell \\' s bill for the reform of the municipal corporations , and with a more liberal election of town councillors ,',\n",
    " 'the chronicles of nougat , volume 2 . by arthur griffiths . section 9 : the first report of the inspector of prisons .',\n",
    " 'newgate has remained rather in the background while the whole of the jails as a body wear under discussion .',\n",
    " 'exchequer , the commissioners of bankruptcy and of taxes smugglers , and a larger number sentence for very short terms ,']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_df.loc[orig_asr_similar['Sample #'].values.tolist()]['original_sentence'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_test = ['printed very few books in this type , 3 only but in their very first book syndrome beginning , with the year 1468 ,',\n",
    " 'the chronicles of nougat , volume to arthur griffiths . section for : new gate down to 1818 .',\n",
    " 'seldom let a session go by without visiting you gate .',\n",
    " 'returns laid before the house of commons showed that 6439 persons have been committed to nougat',\n",
    " 'the number of arrests actually made was 114 , 300 for the kingdom , and 7024 middlesex .',\n",
    " 'there was in the city road a temporary bar , with a collector of tolls who was sometimes on the spot and sometimes not .',\n",
    " 'before dealing with the debtors in newgate , prefer incidentally',\n",
    " 'the best , or at least the most influential prisoners , god lodging in the statehouse , which contained \" eight large handsome rooms . \"',\n",
    " 'in consequence of these disclosures , bambridge and hugging , his predecessor in the office , or committed to newgate ,',\n",
    " 'senators were rather better at the marshalsea .',\n",
    " 'is bequest , which was charged upon his manner at goering , auxins , and hence called the oxford charity ,',\n",
    " 'supreme control of the marshalsea was vested in the marshal of the royal household but although he drew a salary of 500 pounds a year ,',\n",
    " 'neeld found the prisoners in the boro compter ragged , starving , and dirty .',\n",
    " 'the chronicles of nougat , volume 2 . by arthur griffiths . section v : newgate down to 18 18 , part 2 .',\n",
    " 'notes for street , and the poultry , or about 476 and all .',\n",
    " 'mr . davidson , sent to newgate for embezzlement , and whose case is given in the preceding chapter ,',\n",
    " 'will mike mr . bennett right that the condition of the condemned side was the most prominent of the many - fold evils in the prison system of nougat ,',\n",
    " 'it was not strange , therefore , that the inmates of nougat should turn their unoccupied brains and idle hands to all manner of mischief',\n",
    " 'it was very desirable that there should be a more speedy removal of transports from you gate to the ships .',\n",
    " \"mr . green , with stir - fry , mizer ' s . forester , and mr . t . f . buxton , the coadjutor of wilberforce in the great anti - slavery struggle .\",\n",
    " 'specify more particularly one or two of the worst , it may be mentioned that in the boro comforter',\n",
    " 'ilchester the rule of employment have been carried further .',\n",
    " 'the system not adopted generally till nearly half a century later had already prevail that bill chester .',\n",
    " 'godmanchester there was no jail , but cage to secure prisoners till they could be taken before a magistrate .',\n",
    " 'i shall have more to say on this subject , and upon the state of nougat generally , in the following chapter .',\n",
    " 'this committee anniversary strongly upon the system in force at the metropolitan jails , and more especially upon the condition of nougat',\n",
    " 'the committee was appointed , under the presidency of the duke of richmond',\n",
    " 'he blamed the construction of new gate for the neglect of classification , and was yet compelled to confess that he had made no attempt whatever to carry it out .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_test = [\"especially as regards to lower - case letters and type very similar was used during the next 15 or 20 years not only by chauffeur ,\",\n",
    "                \"about the same year mental in at strasburg began to print in a type which is distinctly roman\",\n",
    "                \"and though the famous family of aldis restored its technical excellence , rejecting battered letters ,\",\n",
    "                \"most of caxton ' s zone types of an earlier character ,\",\n",
    "                \"are the leaders in this luckless change , though our own baskerville , who was at work some years before them , went much on the same lines\",\n",
    "                \"now come into general use that are obviously a great improvement on the ordinary \\\" modern style \\\" and use in england , which is in fact the bodony type\",\n",
    "                \"on the top of the jail , continues neeld , ara - watch house and a century - box where two or more guards , with dogs and firearms ,\",\n",
    "                \"these courts were extended to centuries later to several large provincial towns , and all were in full activity when nield road ,\",\n",
    "                \"he had been in the employ of a corn - chandler at islington , and went into london with his master ' s cart and horse .\",\n",
    "                \"shameful malpractices of bambridge ,\",\n",
    "                \"the lord steward of the household , the stewart and officers of the marshalsea court , and others .\",\n",
    "                \"if they happened to be in funds - - among whom was the marquis of slego in 1811 .\",\n",
    "                \"mister . kneeled , a second howard ,\",\n",
    "                \"which became the four george the 4th . tap . 64 , said that he had abstained from legislating for these small jurisdictions \\\" on mature deliberation . \\\"\",\n",
    "                \"nothing was more prominently brought out by the inspectors and the inefficiency of the governor at that time , mister . co .\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_analysis(sample_df, combined_df):\n",
    "    ind = np.array(sample_df['Sample #'].values.tolist())\n",
    "    df = combined_df.loc[ind]\n",
    "    df.insert(2,'Original',sample_df['Original'].values.tolist())\n",
    "    df.insert(5,'ASR',sample_df['ASR'].values.tolist())\n",
    "    df.drop(['original_tags', 'asr_tags'], axis=1, inplace=True)\n",
    "    df.head(50)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern = pattern_analysis(orig_asr_similar, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling(df):\n",
    "    i = 0\n",
    "    equal_length_samples = []\n",
    "    variable_length_samples = []\n",
    "    for sample, original, asr in zip(df.index, \n",
    "                                     df['Original'],\n",
    "                                     df['ASR']):\n",
    "        if len(original) == len(asr):\n",
    "            equal_length_samples.append(sample)\n",
    "        else:\n",
    "            variable_length_samples.append(sample)\n",
    "    equal_length_samples.sort()\n",
    "    variable_length_samples.sort()\n",
    "    equal_length_samples_df = df.loc[equal_length_samples]\n",
    "    variable_length_samples_df = df.loc[variable_length_samples]\n",
    "    return equal_length_samples_df, variable_length_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df, variable_length_words_samples_df = error_sampling(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variable_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variable_length_words_samples_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_words_simulation(sampled_df):\n",
    "    simulated_asr = []\n",
    "    for sample, original_sentence, asr_sentence, original, asr in zip(sampled_df.index,\n",
    "                                     sampled_df['original_sentence'],\n",
    "                                     sampled_df['asr_sentence'],\n",
    "                                     sampled_df['Original'],\n",
    "                                     sampled_df['ASR']):\n",
    "\n",
    "        for x,y in zip(original, asr):\n",
    "            #original_words.append(x)\n",
    "            #asr_words.append(y)\n",
    "            if y in asr_sentence:\n",
    "                asr_sentence = asr_sentence.replace(y, x)\n",
    "            \n",
    "        simulated_asr.append((sample, asr_sentence))\n",
    "    simulated_asr_df = pd.DataFrame(simulated_asr)\n",
    "    return simulated_asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_words_simulation(df):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "            df.index,\n",
    "            df['original_sentence'].values.tolist(),\n",
    "            df['asr_sentence'].values.tolist(),\n",
    "            df['Original'].values.tolist(),\n",
    "            df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        o_label = original_label[original_tag_ind]\n",
    "        for lab in original_tag:\n",
    "            for asr_lab in asr_tag:\n",
    "                local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                if local_error >= 50.0:\n",
    "                    asr_sentence = asr_sentence.replace(asr_lab, lab)\n",
    "        check.append((sample, asr_sentence))\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df(asr_df, simulated_df):\n",
    "    asr_df.loc[simulated_df[0].values.tolist(), 'Sentence'] = simulated_df[1].values.tolist()\n",
    "    return asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = equal_words_simulation(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.loc[simulated_asr_df[0].values.tolist(), 'Sentence'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulated_asr_df = variable_words_simulation(variable_length_words_samples_df)\n",
    "#simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = prepare_model_output(test_df, new_df)\n",
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})\n",
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"ORG\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_similar.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('unprocessed_sampled_original.csv')\n",
    "original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "original = original[:7851]\n",
    "g_original = original.groupby(\"Sentence #\")\n",
    "original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "original_df.reset_index(inplace=True)\n",
    "combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'],\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_analysis(sample_df, combined_df):\n",
    "    ind = np.array(sample_df['Sample #'].values.tolist())\n",
    "    df = combined_df.loc[ind]\n",
    "    df.insert(2,'Original',sample_df['Original'].values.tolist())\n",
    "    df.insert(5,'ASR',sample_df['ASR'].values.tolist())\n",
    "    df.drop(['original_tags', 'asr_tags'], axis=1, inplace=True)\n",
    "    df.head(50)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern = pattern_analysis(orig_asr_similar, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling(df):\n",
    "    i = 0\n",
    "    equal_length_samples = []\n",
    "    variable_length_samples = []\n",
    "    for sample, original, asr in zip(df.index, \n",
    "                                     df['Original'],\n",
    "                                     df['ASR']):\n",
    "        if len(original) == len(asr):\n",
    "            equal_length_samples.append(sample)\n",
    "        else:\n",
    "            variable_length_samples.append(sample)\n",
    "    equal_length_samples.sort()\n",
    "    variable_length_samples.sort()\n",
    "    equal_length_samples_df = df.loc[equal_length_samples]\n",
    "    variable_length_samples_df = df.loc[variable_length_samples]\n",
    "    return equal_length_samples_df, variable_length_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df, variable_length_words_samples_df = error_sampling(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_context(df, n_grams):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "                df.index,\n",
    "                df['original_sentence'].values.tolist(),\n",
    "                df['asr_sentence'].values.tolist(),\n",
    "                df['Original'].values.tolist(),\n",
    "                df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        for l in original_tag_ind:\n",
    "            if l <= (len(original_label)-1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams+1, 1):\n",
    "                    if l+c >= 0:\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l+c < len(original_label):\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "        for l in asr_tag_ind:\n",
    "            if l <= (len(asr_label) - 1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams + 1, 1):\n",
    "                    if l + c >= 0:\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l + c < len(asr_label):\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "        check.append((sample, (\" | \").join(original_bigrams), original_sentence, original_tag, (\" | \").join(asr_bigrams), asr_sentence, asr_tag))\n",
    "    context = pd.DataFrame(check)\n",
    "    context.columns = ['Sample #', 'Original N-Grams', \"original_sentence\", \"Original\", \"ASR N-Grams\", \"asr_sentence\", \"ASR\"]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling3(context):\n",
    "    check = []\n",
    "    for sample, original_ngrams, original_sentence, asr_ngrams, asr_sentence, original_tag, asr_tag in zip(\n",
    "            context['Sample #'].values.tolist(),\n",
    "            context['Original N-Grams'].values.tolist(),\n",
    "            context['original_sentence'].values.tolist(),\n",
    "            context['ASR N-Grams'].values.tolist(),\n",
    "            context['asr_sentence'].values.tolist(),\n",
    "            context['Original'].values.tolist(),\n",
    "            context['ASR'].values.tolist()):\n",
    "\n",
    "        original_ngrams = np.array(original_ngrams.split(\"|\"))\n",
    "        asr_ngrams = np.array(asr_ngrams.split(\"|\"))\n",
    "\n",
    "        local_errors = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for _original in original_tag:\n",
    "            if _original in asr_tag:\n",
    "                if len(asr_ngrams) < len(asr_tag):\n",
    "                    continue\n",
    "                \n",
    "                print(asr_sentence)\n",
    "                asr_sentence = asr_sentence.replace(\"\".join(asr_ngrams[i].rstrip()), \"\".join(original_ngrams[i].rstrip()))\n",
    "                print(asr_sentence)\n",
    "                #print(check)\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "            else:\n",
    "                j = j + 1\n",
    "        check.append((sample, asr_sentence))\n",
    "        print(\"---------------\")\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = finding_context(equal_length_words_samples_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = error_sampling3(context)\n",
    "simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"and though the famous family of aldus restored its technical excellence , rejecting battered letters ,\",\"most of caxton ' s zone types of an earlier character\", \"are the leaders in this luckless change , though our own baskerville , who was at work some years before them , went much on the same lines\",\n",
    "       \"now come into general use that are obviously a great improvement on the ordinary \\\" modern style \\\" and use in england , which is in fact the bodoni type\" , \"on the top of the jail , continues neild , arawatch - house and a century - box , where two or more guards , with dogs and firearms ,\" ,\n",
    "       \"these courts were extended to centuries later to several large provincial towns , and all were in full activity when neild road ,\" , \"he had been in the employ of a corn - chandler at islington , and went into london with his master ' s cart and horse .\",\n",
    "       \"shameful malpractices of bambridge ,\" , \"if they happened to be in funds - - among whom was the marquis of slego in 1811\", \n",
    "       \"mister . neild , a second howard ,\", \"again the 22 charles ii . c20 order the jailer to keep felons and debtors \\\" separate and apart from one another ,\",\n",
    "       \"prisoners were crowded together in the jail , contrary to the requirements of the for george the 4th .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"have now come into general use and are obviously a great improvement on the ordinary \\\" modern style \\\" in use in england , which is in fact the bodoni type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = model_test([test], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
