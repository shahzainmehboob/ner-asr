{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "import Levenshtein\n",
    "import string\n",
    "import difflib\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values = ['O', 'PER', 'LOC', 'ORG']\n",
    "#tag_values = ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_whole_word_mask=True)\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False)\n",
    "model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_lower_case_75.pt\", map_location=torch.device('cpu')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_test(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[:6723]\n",
    "    g_test = df.groupby(\"Sentence #\")\n",
    "    test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    test_df.reset_index(inplace=True)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(data, tokenizer, model):\n",
    "    test = []\n",
    "    #results = open(\"conll03_base_ljspeech_asr_test_without_gpe_uncased_results_lower.txt\", \"a+\")\n",
    "    #test_data=original_data['sentence'].values.tolist()\n",
    "    #test_data=original_sentence\n",
    "    #test_data=test_df['Sentence'].values.tolist()\n",
    "    test_data=data\n",
    "\n",
    "    # ASR TEST DATE LATEST\n",
    "    sentence_no = 0\n",
    "    for data in test_data:\n",
    "        tokenized_sentence = tokenizer.encode(data.lower().strip())\n",
    "        #tokenized_sentence = nlp(data.lower().strip())\n",
    "        input_ids = torch.tensor([tokenized_sentence])\n",
    "        #input_ids = torch.tensor([tokenized_sentence._.trf_word_pieces])\n",
    "\n",
    "        with torch.no_grad():\n",
    "             output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        #tokens = _.trf_word_pieces_\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(tag_values[label_idx])\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            #result = str(sentence_no) + \"\\t\" + label + \"\\t\" + token + \"\\n\"\n",
    "            #results.write(result)\n",
    "            test.append((str(sentence_no), label, token))\n",
    "        sentence_no = sentence_no + 1\n",
    "    test_df = pd.DataFrame(test, columns=['sentence_no', 'labels', 'token'])\n",
    "    return test_df\n",
    "    #test_df.to_csv(\"final_asr_test_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_output(test_df, df):\n",
    "    indexNames = test_df[test_df['token'] == \"[CLS]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    indexNames = test_df[test_df['token'] == \"[SEP]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    test_df['label_asr'] = df['Tag']\n",
    "    test_df['token_asr'] = df['Word']\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(test_df, tags):\n",
    "    new_acc = accuracy_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_acc)\n",
    "\n",
    "    new_f1 = f1_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_f1)\n",
    "    print(\"---STATISTICS ON EACH LABEL---\")\n",
    "    for tag in tags:\n",
    "        true_positive = test_df[((test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_positive))\n",
    "        false_positive = test_df[((test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_positive))\n",
    "        false_negative = test_df[((~test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_negative))\n",
    "        true_negative = test_df[((~test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_negative))\n",
    "        prec = len(true_positive) / (len(true_positive) + len(false_positive))\n",
    "        print(prec)\n",
    "        recall = len(true_positive) / (len(true_positive) + len(false_negative))\n",
    "        print(recall)\n",
    "        f_measure = (2 * prec * recall) / (prec + recall)\n",
    "        print(f_measure)\n",
    "        print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = prepare_data_for_test('unprocessed_sampled_asr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>although</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>took</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #      Word Tag\n",
       "0         2.0       for   O\n",
       "1         2.0  although   O\n",
       "2         2.0       the   O\n",
       "3         2.0   Chinese   O\n",
       "4         2.0      took   O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1693.0</td>\n",
       "      <td>famous female side where the ladies Association still reigned Supreme more system and a greater semblance of decorum was maintained</td>\n",
       "      <td>O,O,O,O,O,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1705.0</td>\n",
       "      <td>the separation of the Sexes was not in deed rigidly carried out in Newgate as yet</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1710.0</td>\n",
       "      <td>could also at any time for tea coffee and sugar to Mrs Brown ' s shop which was inside the female gate</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,PER,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1714.0</td>\n",
       "      <td>some member of the ladies Association observed and commented upon the fact that a young rosy - cheeked girl had been kept by the Governor from transportation</td>\n",
       "      <td>O,O,O,O,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1721.0</td>\n",
       "      <td>committed by the House of Commons who had been lodged in the governor ' s own house</td>\n",
       "      <td>O,O,O,ORG,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #                                                                                                                                                       Sentence                                                        Tag\n",
       "371      1693.0                            famous female side where the ladies Association still reigned Supreme more system and a greater semblance of decorum was maintained                O,O,O,O,O,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O,O,O\n",
       "372      1705.0                                                                              the separation of the Sexes was not in deed rigidly carried out in Newgate as yet                          O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O\n",
       "373      1710.0                                                         could also at any time for tea coffee and sugar to Mrs Brown ' s shop which was inside the female gate              O,O,O,O,O,O,O,O,O,O,O,O,PER,O,O,O,O,O,O,O,O,O\n",
       "374      1714.0  some member of the ladies Association observed and commented upon the fact that a young rosy - cheeked girl had been kept by the Governor from transportation  O,O,O,O,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O\n",
       "375      1721.0                                                                            committed by the House of Commons who had been lodged in the governor ' s own house                    O,O,O,ORG,ORG,ORG,O,O,O,O,O,O,O,O,O,O,O"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(test_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>labels</th>\n",
       "      <th>token</th>\n",
       "      <th>label_asr</th>\n",
       "      <th>token_asr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6623</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>side</td>\n",
       "      <td>O</td>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6624</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>where</td>\n",
       "      <td>O</td>\n",
       "      <td>where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6625</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>ladies</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ladies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6627</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>association</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6628</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>still</td>\n",
       "      <td>O</td>\n",
       "      <td>still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6629</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>reigned</td>\n",
       "      <td>O</td>\n",
       "      <td>reigned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6630</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>supreme</td>\n",
       "      <td>O</td>\n",
       "      <td>Supreme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6631</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>more</td>\n",
       "      <td>O</td>\n",
       "      <td>more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>system</td>\n",
       "      <td>O</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6633</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6634</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6635</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>greater</td>\n",
       "      <td>O</td>\n",
       "      <td>greater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6636</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>semblance</td>\n",
       "      <td>O</td>\n",
       "      <td>semblance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6637</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6638</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>decorum</td>\n",
       "      <td>O</td>\n",
       "      <td>decorum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6639</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6640</th>\n",
       "      <td>371</td>\n",
       "      <td>O</td>\n",
       "      <td>maintained</td>\n",
       "      <td>O</td>\n",
       "      <td>maintained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6641</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6642</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>separation</td>\n",
       "      <td>O</td>\n",
       "      <td>separation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6643</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6645</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>sexes</td>\n",
       "      <td>O</td>\n",
       "      <td>Sexes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6646</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6647</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>not</td>\n",
       "      <td>O</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6648</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6649</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>deed</td>\n",
       "      <td>O</td>\n",
       "      <td>deed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6650</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>rigidly</td>\n",
       "      <td>O</td>\n",
       "      <td>rigidly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6651</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>carried</td>\n",
       "      <td>O</td>\n",
       "      <td>carried</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6652</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>out</td>\n",
       "      <td>O</td>\n",
       "      <td>out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6653</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6654</th>\n",
       "      <td>372</td>\n",
       "      <td>LOC</td>\n",
       "      <td>newgate</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Newgate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6655</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6656</th>\n",
       "      <td>372</td>\n",
       "      <td>O</td>\n",
       "      <td>yet</td>\n",
       "      <td>O</td>\n",
       "      <td>yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6657</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>could</td>\n",
       "      <td>O</td>\n",
       "      <td>could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6658</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>also</td>\n",
       "      <td>O</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6659</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>at</td>\n",
       "      <td>O</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6660</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>any</td>\n",
       "      <td>O</td>\n",
       "      <td>any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>time</td>\n",
       "      <td>O</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6662</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6663</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>tea</td>\n",
       "      <td>O</td>\n",
       "      <td>tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>coffee</td>\n",
       "      <td>O</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6665</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>sugar</td>\n",
       "      <td>O</td>\n",
       "      <td>sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6667</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>mrs</td>\n",
       "      <td>O</td>\n",
       "      <td>Mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>373</td>\n",
       "      <td>PER</td>\n",
       "      <td>brown</td>\n",
       "      <td>PER</td>\n",
       "      <td>Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6670</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>'</td>\n",
       "      <td>O</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>s</td>\n",
       "      <td>O</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6672</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>shop</td>\n",
       "      <td>O</td>\n",
       "      <td>shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6673</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>which</td>\n",
       "      <td>O</td>\n",
       "      <td>which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6674</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6675</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>inside</td>\n",
       "      <td>O</td>\n",
       "      <td>inside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6676</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6677</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>female</td>\n",
       "      <td>O</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6678</th>\n",
       "      <td>373</td>\n",
       "      <td>O</td>\n",
       "      <td>gate</td>\n",
       "      <td>O</td>\n",
       "      <td>gate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>some</td>\n",
       "      <td>O</td>\n",
       "      <td>some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>member</td>\n",
       "      <td>O</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6683</th>\n",
       "      <td>374</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ladies</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ladies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6684</th>\n",
       "      <td>374</td>\n",
       "      <td>ORG</td>\n",
       "      <td>association</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6685</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>observed</td>\n",
       "      <td>O</td>\n",
       "      <td>observed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6686</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6687</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>commented</td>\n",
       "      <td>O</td>\n",
       "      <td>commented</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6688</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>upon</td>\n",
       "      <td>O</td>\n",
       "      <td>upon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6689</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>fact</td>\n",
       "      <td>O</td>\n",
       "      <td>fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>young</td>\n",
       "      <td>O</td>\n",
       "      <td>young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>rosy</td>\n",
       "      <td>O</td>\n",
       "      <td>rosy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6696</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>cheeked</td>\n",
       "      <td>O</td>\n",
       "      <td>cheeked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6697</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>girl</td>\n",
       "      <td>O</td>\n",
       "      <td>girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6698</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6699</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>been</td>\n",
       "      <td>O</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>kept</td>\n",
       "      <td>O</td>\n",
       "      <td>kept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6701</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>by</td>\n",
       "      <td>O</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6702</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6703</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>governor</td>\n",
       "      <td>O</td>\n",
       "      <td>Governor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6704</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>from</td>\n",
       "      <td>O</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6705</th>\n",
       "      <td>374</td>\n",
       "      <td>O</td>\n",
       "      <td>transportation</td>\n",
       "      <td>O</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6706</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>committed</td>\n",
       "      <td>O</td>\n",
       "      <td>committed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6707</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>by</td>\n",
       "      <td>O</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6708</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>375</td>\n",
       "      <td>ORG</td>\n",
       "      <td>house</td>\n",
       "      <td>ORG</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6710</th>\n",
       "      <td>375</td>\n",
       "      <td>ORG</td>\n",
       "      <td>of</td>\n",
       "      <td>ORG</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6711</th>\n",
       "      <td>375</td>\n",
       "      <td>ORG</td>\n",
       "      <td>commons</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Commons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6712</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>who</td>\n",
       "      <td>O</td>\n",
       "      <td>who</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6713</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6714</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>been</td>\n",
       "      <td>O</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6715</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>lodged</td>\n",
       "      <td>O</td>\n",
       "      <td>lodged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6716</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6718</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>governor</td>\n",
       "      <td>O</td>\n",
       "      <td>governor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6719</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>'</td>\n",
       "      <td>O</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6720</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>s</td>\n",
       "      <td>O</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>own</td>\n",
       "      <td>O</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>375</td>\n",
       "      <td>O</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_no labels           token label_asr       token_asr\n",
       "6623         371      O            side         O            side\n",
       "6624         371      O           where         O           where\n",
       "6625         371      O             the         O             the\n",
       "6626         371      O          ladies       ORG          ladies\n",
       "6627         371      O     association       ORG     Association\n",
       "6628         371      O           still         O           still\n",
       "6629         371      O         reigned         O         reigned\n",
       "6630         371      O         supreme         O         Supreme\n",
       "6631         371      O            more         O            more\n",
       "6632         371      O          system         O          system\n",
       "6633         371      O             and         O             and\n",
       "6634         371      O               a         O               a\n",
       "6635         371      O         greater         O         greater\n",
       "6636         371      O       semblance         O       semblance\n",
       "6637         371      O              of         O              of\n",
       "6638         371      O         decorum         O         decorum\n",
       "6639         371      O             was         O             was\n",
       "6640         371      O      maintained         O      maintained\n",
       "6641         372      O             the         O             the\n",
       "6642         372      O      separation         O      separation\n",
       "6643         372      O              of         O              of\n",
       "6644         372      O             the         O             the\n",
       "6645         372      O           sexes         O           Sexes\n",
       "6646         372      O             was         O             was\n",
       "6647         372      O             not         O             not\n",
       "6648         372      O              in         O              in\n",
       "6649         372      O            deed         O            deed\n",
       "6650         372      O         rigidly         O         rigidly\n",
       "6651         372      O         carried         O         carried\n",
       "6652         372      O             out         O             out\n",
       "6653         372      O              in         O              in\n",
       "6654         372    LOC         newgate       LOC         Newgate\n",
       "6655         372      O              as         O              as\n",
       "6656         372      O             yet         O             yet\n",
       "6657         373      O           could         O           could\n",
       "6658         373      O            also         O            also\n",
       "6659         373      O              at         O              at\n",
       "6660         373      O             any         O             any\n",
       "6661         373      O            time         O            time\n",
       "6662         373      O             for         O             for\n",
       "6663         373      O             tea         O             tea\n",
       "6664         373      O          coffee         O          coffee\n",
       "6665         373      O             and         O             and\n",
       "6666         373      O           sugar         O           sugar\n",
       "6667         373      O              to         O              to\n",
       "6668         373      O             mrs         O             Mrs\n",
       "6669         373    PER           brown       PER           Brown\n",
       "6670         373      O               '         O               '\n",
       "6671         373      O               s         O               s\n",
       "6672         373      O            shop         O            shop\n",
       "6673         373      O           which         O           which\n",
       "6674         373      O             was         O             was\n",
       "6675         373      O          inside         O          inside\n",
       "6676         373      O             the         O             the\n",
       "6677         373      O          female         O          female\n",
       "6678         373      O            gate         O            gate\n",
       "6679         374      O            some         O            some\n",
       "6680         374      O          member         O          member\n",
       "6681         374      O              of         O              of\n",
       "6682         374      O             the         O             the\n",
       "6683         374    ORG          ladies       ORG          ladies\n",
       "6684         374    ORG     association       ORG     Association\n",
       "6685         374      O        observed         O        observed\n",
       "6686         374      O             and         O             and\n",
       "6687         374      O       commented         O       commented\n",
       "6688         374      O            upon         O            upon\n",
       "6689         374      O             the         O             the\n",
       "6690         374      O            fact         O            fact\n",
       "6691         374      O            that         O            that\n",
       "6692         374      O               a         O               a\n",
       "6693         374      O           young         O           young\n",
       "6694         374      O            rosy         O            rosy\n",
       "6695         374      O               -         O               -\n",
       "6696         374      O         cheeked         O         cheeked\n",
       "6697         374      O            girl         O            girl\n",
       "6698         374      O             had         O             had\n",
       "6699         374      O            been         O            been\n",
       "6700         374      O            kept         O            kept\n",
       "6701         374      O              by         O              by\n",
       "6702         374      O             the         O             the\n",
       "6703         374      O        governor         O        Governor\n",
       "6704         374      O            from         O            from\n",
       "6705         374      O  transportation         O  transportation\n",
       "6706         375      O       committed         O       committed\n",
       "6707         375      O              by         O              by\n",
       "6708         375      O             the         O             the\n",
       "6709         375    ORG           house       ORG           House\n",
       "6710         375    ORG              of       ORG              of\n",
       "6711         375    ORG         commons       ORG         Commons\n",
       "6712         375      O             who         O             who\n",
       "6713         375      O             had         O             had\n",
       "6714         375      O            been         O            been\n",
       "6715         375      O          lodged         O          lodged\n",
       "6716         375      O              in         O              in\n",
       "6717         375      O             the         O             the\n",
       "6718         375      O        governor         O        governor\n",
       "6719         375      O               '         O               '\n",
       "6720         375      O               s         O               s\n",
       "6721         375      O             own         O             own\n",
       "6722         375      O           house         O           house"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'LOC', 'PER', 'ORG'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label_asr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_tag</th>\n",
       "      <th>asr_tag</th>\n",
       "      <th>asr_sentence_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, O, O, LOC, O, LOC, LOC, O, O, O]</td>\n",
       "      <td>[O, O, O, LOC, O, LOC, LOC, O, O, O]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, LOC, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, LOC, O, O, O, O, O]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    model_tag                                                                     asr_tag  asr_sentence_no\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]                0\n",
       "1             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]                1\n",
       "2       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]                2\n",
       "3                                        [O, O, O, LOC, O, LOC, LOC, O, O, O]                                        [O, O, O, LOC, O, LOC, LOC, O, O, O]                3\n",
       "4                                      [O, O, O, O, O, O, LOC, O, O, O, O, O]                                      [O, O, O, O, O, O, LOC, O, O, O, O, O]                4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9535921463632308\n",
      "F1 Score:  0.8026666666666668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_analysis(test_df, original_data_path):\n",
    "    g_asr = test_df.groupby(\"sentence_no\")\n",
    "    asr_df = pd.DataFrame({'Sentence': g_asr.apply(lambda sdf: \" \".join(map(str,sdf.token))),\n",
    "                      'Tag': g_asr.apply(lambda sdf: \",\".join(sdf.labels))})\n",
    "    asr_df['asr_sentence_no'] = asr_df.index\n",
    "    asr_df[[\"asr_sentence_no\"]] = asr_df[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "    asr_df.sort_values('asr_sentence_no', inplace=True)\n",
    "    asr_df.reset_index(drop=True, inplace=True)\n",
    "    original = pd.read_csv(original_data_path)\n",
    "    original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    original = original[:7851]\n",
    "    g_original = original.groupby(\"Sentence #\")\n",
    "    original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    original_df.reset_index(inplace=True)\n",
    "    combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'].str.lower(),\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})\n",
    "    return asr_df, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_finding(tag, combined_df):\n",
    "#tag = \"PER\"\n",
    "    analysis = []\n",
    "    for i in range(0, len(combined_df), 1):\n",
    "        sample = combined_df.loc[[i]]\n",
    "        for original_sentence, asr_sentence, original_tag, asr_tag in zip(sample['original_sentence'].values.tolist(),\n",
    "                                                                          sample['asr_sentence'].values.tolist(),\n",
    "                                                                          sample['original_tags'].values.tolist(),\n",
    "                                                                          sample['asr_tags'].values.tolist()):\n",
    "            original_tag_token = np.array(original_tag.split(\",\"))\n",
    "            asr_tag_token = np.array(asr_tag.split(\",\"))\n",
    "            original_label = np.array(original_sentence.lower().split())\n",
    "            asr_label = np.array(asr_sentence.lower().split())\n",
    "\n",
    "            if tag in original_tag_token:\n",
    "                original_tag_ind = [index for index, element in enumerate(original_tag_token) if\n",
    "                                    original_tag_token[index] == tag]\n",
    "                if tag in asr_tag_token:\n",
    "                    asr_tag_ind = [index for index, element in enumerate(asr_tag_token) if\n",
    "                                       asr_tag_token[index] == tag]\n",
    "                    \n",
    "                    asr_tokens = []\n",
    "                    original_tokens = []\n",
    "                    errors = []\n",
    "                        # Sweynheim pannartz\n",
    "                        # Swain heim pannartz\n",
    "                    for ind in original_tag_ind:\n",
    "                        original_entity = original_label[ind]\n",
    "                        asr_entity = difflib.get_close_matches(original_entity, asr_label[asr_tag_ind])\n",
    "                        if len(asr_entity) > 0:\n",
    "                            asr_entity = asr_entity[0]\n",
    "                            error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(len(original_entity), len(asr_entity)))) * 100\n",
    "                            if error >= 50:\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(\"None\")\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        else:\n",
    "                            asr_tokens.append(\"None\")\n",
    "                            original_tokens.append(original_entity)\n",
    "                            errors.append(0.0)\n",
    "                    analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), True))\n",
    "                else:\n",
    "                    check = []\n",
    "                    o_label = original_label[original_tag_ind]\n",
    "                    for lab in o_label:\n",
    "                        j = 0\n",
    "                        for asr_lab in asr_label:\n",
    "                            local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                            if local_error >= 50.0:\n",
    "                                check.append(j)\n",
    "                            j = j + 1\n",
    "                    if len(check) > 0:\n",
    "                        asr_tokens = []\n",
    "                        original_tokens = []\n",
    "                        errors = []\n",
    "                        for ind in original_tag_ind:\n",
    "                            original_entity = original_label[ind]\n",
    "                            asr_entity = difflib.get_close_matches(original_entity, asr_label[check])\n",
    "                            if len(asr_entity) > 0:\n",
    "                                asr_entity = asr_entity[0]\n",
    "                                error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(\n",
    "                                len(original_entity), len(asr_entity)))) * 100\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(\"None\")\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), False))\n",
    "                    else:\n",
    "                        analysis.append((i, original_label[original_tag_ind], [\"None\"], [0.0], 0.0, False))\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein', 'Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([100.0,100.0]) == 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_nofound.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_analysis(sample_df, combined_df):\n",
    "    ind = np.array(sample_df['Sample #'].values.tolist())\n",
    "    df = combined_df.loc[ind]\n",
    "    df.insert(2,'Original',sample_df['Original'].values.tolist())\n",
    "    df.insert(5,'ASR',sample_df['ASR'].values.tolist())\n",
    "    df.drop(['original_tags', 'asr_tags'], axis=1, inplace=True)\n",
    "    df.head(50)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern = pattern_analysis(orig_asr_similar, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling(df):\n",
    "    i = 0\n",
    "    equal_length_samples = []\n",
    "    variable_length_samples = []\n",
    "    for sample, original, asr in zip(df.index, \n",
    "                                     df['Original'],\n",
    "                                     df['ASR']):\n",
    "        if len(original) == len(asr):\n",
    "            equal_length_samples.append(sample)\n",
    "        else:\n",
    "            variable_length_samples.append(sample)\n",
    "    equal_length_samples.sort()\n",
    "    variable_length_samples.sort()\n",
    "    equal_length_samples_df = df.loc[equal_length_samples]\n",
    "    variable_length_samples_df = df.loc[variable_length_samples]\n",
    "    return equal_length_samples_df, variable_length_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df, variable_length_words_samples_df = error_sampling(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variable_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variable_length_words_samples_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_words_simulation(sampled_df):\n",
    "    simulated_asr = []\n",
    "    for sample, original_sentence, asr_sentence, original, asr in zip(sampled_df.index,\n",
    "                                     sampled_df['original_sentence'],\n",
    "                                     sampled_df['asr_sentence'],\n",
    "                                     sampled_df['Original'],\n",
    "                                     sampled_df['ASR']):\n",
    "\n",
    "        for x,y in zip(original, asr):\n",
    "            #original_words.append(x)\n",
    "            #asr_words.append(y)\n",
    "            if y in asr_sentence:\n",
    "                asr_sentence = asr_sentence.replace(y, x)\n",
    "            \n",
    "        simulated_asr.append((sample, asr_sentence))\n",
    "    simulated_asr_df = pd.DataFrame(simulated_asr)\n",
    "    return simulated_asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_words_simulation(df):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "            df.index,\n",
    "            df['original_sentence'].values.tolist(),\n",
    "            df['asr_sentence'].values.tolist(),\n",
    "            df['Original'].values.tolist(),\n",
    "            df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        o_label = original_label[original_tag_ind]\n",
    "        for lab in original_tag:\n",
    "            for asr_lab in asr_tag:\n",
    "                local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                if local_error >= 50.0:\n",
    "                    asr_sentence = asr_sentence.replace(asr_lab, lab)\n",
    "        check.append((sample, asr_sentence))\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df(asr_df, simulated_df):\n",
    "    asr_df.loc[simulated_df[0].values.tolist(), 'Sentence'] = simulated_df[1].values.tolist()\n",
    "    return asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = equal_words_simulation(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulated_asr_df = variable_words_simulation(variable_length_words_samples_df)\n",
    "#simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = prepare_model_output(test_df, new_df)\n",
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_similar.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_simulated_df = combined_df.loc[orig_asr_similar['Sample #'].values.tolist(),['original_sentence','original_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_simulated_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = df.groupby(\"Sentence #\")\n",
    "x = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no = list(range(0, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.index = sentence_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[context_simulated_df.index.tolist(), 'Sentence'] = context_simulated_df['original_sentence'].values.tolist()\n",
    "x.loc[context_simulated_df.index.tolist(), 'Tag'] = context_simulated_df['original_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_simulated_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.loc[context_simulated_df.index.tolist(), 'Sentence'] = context_simulated_df['original_sentence'].values.tolist()\n",
    "asr_df.loc[context_simulated_df.index.tolist(), 'Tag'] = context_simulated_df['original_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no = 0\n",
    "dataset=[]\n",
    "for sentences, tags in zip(x['Sentence'].values.tolist(), x['Tag'].values.tolist()):\n",
    "    sentence=sentences.split(\" \")\n",
    "    tag = tags.split(\",\")\n",
    "    for word, label in zip(sentence, tag):\n",
    "        dataset.append((sentence_no, word, label))\n",
    "    sentence_no = sentence_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(dataset, columns=['Sentence #', 'Word', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"ORG\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_context(df, n_grams):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "                df.index,\n",
    "                df['original_sentence'].values.tolist(),\n",
    "                df['asr_sentence'].values.tolist(),\n",
    "                df['Original'].values.tolist(),\n",
    "                df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        for l in original_tag_ind:\n",
    "            if l <= (len(original_label)-1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams+1, 1):\n",
    "                    if l+c >= 0:\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l+c < len(original_label):\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "        for l in asr_tag_ind:\n",
    "            if l <= (len(asr_label) - 1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams + 1, 1):\n",
    "                    if l + c >= 0:\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l + c < len(asr_label):\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "        check.append((sample, (\" | \").join(original_bigrams), original_sentence, original_tag, (\" | \").join(asr_bigrams), asr_sentence, asr_tag))\n",
    "    context = pd.DataFrame(check)\n",
    "    context.columns = ['Sample #', 'Original N-Grams', \"original_sentence\", \"Original\", \"ASR N-Grams\", \"asr_sentence\", \"ASR\"]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling3(context):\n",
    "    check = []\n",
    "    for sample, original_ngrams, original_sentence, asr_ngrams, asr_sentence, original_tag, asr_tag in zip(\n",
    "            context['Sample #'].values.tolist(),\n",
    "            context['Original N-Grams'].values.tolist(),\n",
    "            context['original_sentence'].values.tolist(),\n",
    "            context['ASR N-Grams'].values.tolist(),\n",
    "            context['asr_sentence'].values.tolist(),\n",
    "            context['Original'].values.tolist(),\n",
    "            context['ASR'].values.tolist()):\n",
    "\n",
    "        original_ngrams = np.array(original_ngrams.split(\"|\"))\n",
    "        asr_ngrams = np.array(asr_ngrams.split(\"|\"))\n",
    "\n",
    "        local_errors = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for _original in original_tag:\n",
    "            if _original in asr_tag:\n",
    "                if len(asr_ngrams) < len(asr_tag):\n",
    "                    continue\n",
    "                \n",
    "                print(asr_sentence)\n",
    "                asr_sentence = asr_sentence.replace(\"\".join(asr_ngrams[i].rstrip()), \"\".join(original_ngrams[i].rstrip()))\n",
    "                print(asr_sentence)\n",
    "                #print(check)\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "            else:\n",
    "                j = j + 1\n",
    "        check.append((sample, asr_sentence))\n",
    "        print(\"---------------\")\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = finding_context(equal_length_words_samples_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = error_sampling3(context)\n",
    "simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = finding_context(variable_length_words_samples_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = error_sampling3(context)\n",
    "simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"and though the famous family of aldus restored its technical excellence , rejecting battered letters ,\",\"most of caxton ' s zone types of an earlier character\", \"are the leaders in this luckless change , though our own baskerville , who was at work some years before them , went much on the same lines\",\n",
    "       \"now come into general use that are obviously a great improvement on the ordinary \\\" modern style \\\" and use in england , which is in fact the bodoni type\" , \"on the top of the jail , continues neild , arawatch - house and a century - box , where two or more guards , with dogs and firearms ,\" ,\n",
    "       \"these courts were extended to centuries later to several large provincial towns , and all were in full activity when neild road ,\" , \"he had been in the employ of a corn - chandler at islington , and went into london with his master ' s cart and horse .\",\n",
    "       \"shameful malpractices of bambridge ,\" , \"if they happened to be in funds - - among whom was the marquis of slego in 1811\", \n",
    "       \"mister . neild , a second howard ,\", \"again the 22 charles ii . c20 order the jailer to keep felons and debtors \\\" separate and apart from one another ,\",\n",
    "       \"prisoners were crowded together in the jail , contrary to the requirements of the for george the 4th .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"have now come into general use and are obviously a great improvement on the ordinary \\\" modern style \\\" in use in england , which is in fact the bodoni type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = model_test([test], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
