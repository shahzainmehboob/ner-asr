{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "import Levenshtein\n",
    "import string\n",
    "import difflib\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_values = ['O', 'PER', 'LOC', 'ORG']\n",
    "#tag_values = ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_whole_word_mask=True)\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False)\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_50_1.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_50_2.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_50_3.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_50_4.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_50_5.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_50_1.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_50_2.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_50_3.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_50_4.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_50_5.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_lower_case_75_1.pt\", map_location=torch.device('cpu')), strict=False)\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_without_punctuation_with_broken_entities2_75.pt\", map_location=torch.device('cpu')), strict=False) #F1 Score:  0.8054794520547945\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_75.pt\", map_location=torch.device('cpu'))) #0.802\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_broken_entities_75_1.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_without_punctuation_with_broken_entities_75.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_without_punctuation_with_broken_entities_75_1.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_without_punctuation_with_broken_entities_75_2.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_with_punctuation_with_broken_entities_75_mse_2.pt\", map_location=torch.device('cpu')))#F1 Score:  0.7848101265822784\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_with_punctuation_with_broken_entities_75_without_mse_1.pt\", map_location=torch.device('cpu')))#F1 Score:  0.786013986013986\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_with_punctuation_with_broken_entities_75_with_mse_3.pt\", map_location=torch.device('cpu')))#F1 Score:  0.7019498607242339\n",
    "#model.load_state_dict(torch.load(\"../model/test_new_bert_base_conll_with_punctuation_with_broken_entities_75_with_mse_3.pt\", map_location=torch.device('cpu')))#Sequential F1 Score:  0.791891891891892\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_with_punctuation_with_broken_entities_concatenation_75_with_mse_3.pt\", map_location=torch.device('cpu')))#Sequential\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_with_punctuation_with_all_broken_entities_concatenation_75_with_mse.pt\", map_location=torch.device('cpu')))#Sequential\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_with_punctuation_with_all_broken_entities_50.pt\", map_location=torch.device('cpu')))#Sequential\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_without_punctuation_with_all_broken_entities_50_without_mse.pt\", map_location=torch.device('cpu')))#Sequential\n",
    "#model.load_state_dict(torch.load(\"../model/bert_base_conll_without_punctuation_with_all_broken_entities_50_without_mse_concatenation.pt\", map_location=torch.device('cpu')))#Sequential.pt\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_with_punctuation_with_all_broken_entities_concatenation_75_without_mse.pt\", map_location=torch.device('cpu')))#Sequential\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"/Volumes/My Passport/thesis/model/with_punctuation_with_all_broken_entities_concatenation_100_with_consistency_loss.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"/Volumes/My Passport/thesis/model/with_punctuation_with_all_broken_entities_horizontal_concatenation_with_mse_50.pt\", map_location=torch.device('cpu'))) #F1 Score:  0.6389351081530782\n",
    "#model.load_state_dict(torch.load(\"/Volumes/My Passport/thesis/model/with_punctuation_with_all_broken_entities_horizontal_concatenation_50.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"/Volumes/My Passport/thesis/model/without_punctuation_with_all_broken_entities_horizontal_concatenation_50.pt\", map_location=torch.device('cpu'))) #F1 Score:  0.7939142461964039\n",
    "#model.load_state_dict(torch.load(\"/Volumes/My Passport/thesis/model/without_punctuation_with_all_broken_entities_concatenation_50_with_consistency_loss.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"/Volumes/My Passport/thesis/model/without_punctuation_with_all_broken_entities_horizontal_concatenation_with_mse_50.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/with_punctuation_with_all_broken_entities_vertical_concatenation_100.pt\", map_location=torch.device('cpu'))) #F1 Score:  0.8436268068331143\n",
    "#model.load_state_dict(torch.load(\"../model/without_punctuation_with_all_broken_entities_vertical_concatenation_100.pt\", map_location=torch.device('cpu'))) #F1 Score:  0.7716312056737589\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/without_punctuation_with_all_broken_entities_horizontal_concatenation_with_mse_50_1.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/without_punctuation_with_all_broken_entities_horizontal_concatenation_with_mse_50_2_1.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/without_punctuation_with_all_broken_entities_horizontal_concatenation_with_mse_50_2.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/without_punctuation_with_all_broken_entities_horizontal_concatenation_with_mse_50_3.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/without_punctuation_with_all_broken_entities_horizontal_concatenation_with_mse_50_4.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/without_punctuation_with_all_broken_entities_horizontal_concatenation_with_mse_50_5.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/with_punctuation_with_all_broken_entities_vertical_concatenation_100_1.pt\", map_location=torch.device('cpu')))\n",
    "model.load_state_dict(torch.load(\"../model/with_punctuation_with_all_broken_entities_vertical_concatenation_100_2.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/with_punctuation_with_all_broken_entities_vertical_concatenation_100_3.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/with_punctuation_with_all_broken_entities_vertical_concatenation_100_4.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/with_punctuation_with_all_broken_entities_vertical_concatenation_100_5.pt\", map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_without_punctuation_with_broken_entities_75_3.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_without_punctuation_with_broken_entities_75_4.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/new_bert_base_conll_without_punctuation_with_broken_entities_traindataloader_75_5.pt\", map_location=torch.device('cpu')))\n",
    "#model.load_state_dict(torch.load(\"../model/new_conll_train_preprocessed_without_punctuation_with_broken_entities_broken_context_75.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_test(filepath):\n",
    "    df = pd.read_csv(filepath, sep=\";\")\n",
    "    #df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    #df = df[:6723]\n",
    "    g_test = df.groupby(\"Sentence #\")\n",
    "    test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    test_df.reset_index(inplace=True)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_original_data_for_test(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    #df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df = df[:7851]\n",
    "    g_test = df.groupby(\"Sentence #\")\n",
    "    test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    test_df.reset_index(inplace=True)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_conll_data_for_test(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    g_test = df.groupby(\"Sentence #\")\n",
    "    test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    test_df.reset_index(inplace=True)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(data, tokenizer, model):\n",
    "    test = []\n",
    "    #results = open(\"conll03_base_ljspeech_asr_test_without_gpe_uncased_results_lower.txt\", \"a+\")\n",
    "    #test_data=original_data['sentence'].values.tolist()\n",
    "    #test_data=original_sentence\n",
    "    #test_data=test_df['Sentence'].values.tolist()\n",
    "    test_data=data\n",
    "\n",
    "    # ASR TEST DATE LATEST\n",
    "    sentence_no = 0\n",
    "    for data in test_data:\n",
    "        tokenized_sentence = tokenizer.encode(data.lower().strip())\n",
    "        #tokenized_sentence = nlp(data.lower().strip())\n",
    "        input_ids = torch.tensor([tokenized_sentence])\n",
    "        #input_ids = torch.tensor([tokenized_sentence._.trf_word_pieces])\n",
    "\n",
    "        with torch.no_grad():\n",
    "             output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        #tokens = _.trf_word_pieces_\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(tag_values[label_idx])\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            #result = str(sentence_no) + \"\\t\" + label + \"\\t\" + token + \"\\n\"\n",
    "            #results.write(result)\n",
    "            test.append((str(sentence_no), label, token))\n",
    "        sentence_no = sentence_no + 1\n",
    "    test_df = pd.DataFrame(test, columns=['sentence_no', 'labels', 'token'])\n",
    "    return test_df\n",
    "    #test_df.to_csv(\"final_asr_test_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_output(test_df, df):\n",
    "    indexNames = test_df[test_df['token'] == \"[CLS]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    indexNames = test_df[test_df['token'] == \"[SEP]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    test_df['label_asr'] = df['Tag']\n",
    "    test_df['token_asr'] = df['Word']\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(test_df, tags):\n",
    "    new_acc = accuracy_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_acc)\n",
    "\n",
    "    new_f1 = f1_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_f1)\n",
    "    print(\"---STATISTICS ON EACH LABEL---\")\n",
    "    for tag in tags:\n",
    "        true_positive = test_df[((test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_positive))\n",
    "        false_positive = test_df[((test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_positive))\n",
    "        false_negative = test_df[((~test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_negative))\n",
    "        true_negative = test_df[((~test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_negative))\n",
    "        prec = len(true_positive) / (len(true_positive) + len(false_positive))\n",
    "        print(prec)\n",
    "        recall = len(true_positive) / (len(true_positive) + len(false_negative))\n",
    "        print(recall)\n",
    "        f_measure = (2 * prec * recall) / (prec + recall)\n",
    "        print(f_measure)\n",
    "        print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = prepare_data_for_test('unprocessed_sampled_asr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = prepare_original_data_for_test('unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = prepare_conll_data_for_test('conll_test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>for although the Chinese took Impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands by a similar process</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>what the first Bible actually dated which also was printed at mace by Peter Shaffer in the year 1462</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,LOC,O,PER,PER,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>especially as regards to lowercase letters and type very similar was used during the next 15 or 20 years not only by chauffeur</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>Buy printers in Strasburg basil Paris Lubec and other cities</td>\n",
       "      <td>O,O,O,LOC,O,LOC,LOC,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>but don ' t expect in Italy letter with most often used</td>\n",
       "      <td>O,O,O,O,O,O,LOC,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #                                                                                                                                                    Sentence                                                Tag\n",
       "0         2.0  for although the Chinese took Impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands by a similar process  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O\n",
       "1        23.0                                                        what the first Bible actually dated which also was printed at mace by Peter Shaffer in the year 1462        O,O,O,O,O,O,O,O,O,O,O,LOC,O,PER,PER,O,O,O,O\n",
       "2        26.0                              especially as regards to lowercase letters and type very similar was used during the next 15 or 20 years not only by chauffeur    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER\n",
       "3        27.0                                                                                                Buy printers in Strasburg basil Paris Lubec and other cities                          O,O,O,LOC,O,LOC,LOC,O,O,O\n",
       "4        28.0                                                                                                     but don ' t expect in Italy letter with most often used                          O,O,O,O,O,O,LOC,O,O,O,O,O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for sentence_no, word, tag in zip(df['Sentence #'].values.tolist(), df['Word'].values.tolist(), df['Tag'].values.tolist()):\n",
    "    if word in string.punctuation:\n",
    "        continue\n",
    "    else:\n",
    "        dataset.append((sentence_no, word, tag))\n",
    "df = pd.DataFrame(dataset, columns=['Sentence #', 'Word', 'Tag'])\n",
    "g_test = df.groupby(\"Sentence #\")\n",
    "test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>12126.0</td>\n",
       "      <td>stating that he had distributed its pamphlets on the streets of Dallas this information did not reach agent hosty in Dallas until June</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O,O,O,O,O,LOC,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>12168.0</td>\n",
       "      <td>and that they had been married in Fort Worth and live there until coming to New Orleans he had told the New Orleans arresting officers that he had been born in Cuba</td>\n",
       "      <td>O,O,O,O,O,O,O,LOC,LOC,O,O,O,O,O,O,LOC,LOC,O,O,O,O,LOC,LOC,O,O,O,O,O,O,O,O,LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>12198.0</td>\n",
       "      <td>the FBI representative in Mexico City arranged to follow up this information with the CIA and to verify Oswald entry into Mexico</td>\n",
       "      <td>O,ORG,O,O,LOC,O,O,O,O,O,O,O,O,O,ORG,O,O,O,PER,O,O,LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>12234.0</td>\n",
       "      <td>he had received a copy of the report of the New Orleans office which contained agent Quigley ' s memorandum of the interview in the New Orleans jail on August 10th</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,LOC,LOC,O,O,O,O,PER,O,O,O,O,O,O,O,O,LOC,LOC,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>12536.0</td>\n",
       "      <td>two of the agents returned to their rooms the seven others proceeded to an establishment called The Cellar Coffee House</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,ORG,ORG,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #                                                                                                                                                              Sentence                                                                            Tag\n",
       "468     12126.0                                stating that he had distributed its pamphlets on the streets of Dallas this information did not reach agent hosty in Dallas until June                              O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O,O,O,O,O,LOC,O,O\n",
       "469     12168.0  and that they had been married in Fort Worth and live there until coming to New Orleans he had told the New Orleans arresting officers that he had been born in Cuba  O,O,O,O,O,O,O,LOC,LOC,O,O,O,O,O,O,LOC,LOC,O,O,O,O,LOC,LOC,O,O,O,O,O,O,O,O,LOC\n",
       "470     12198.0                                      the FBI representative in Mexico City arranged to follow up this information with the CIA and to verify Oswald entry into Mexico                          O,ORG,O,O,LOC,O,O,O,O,O,O,O,O,O,ORG,O,O,O,PER,O,O,LOC\n",
       "471     12234.0   he had received a copy of the report of the New Orleans office which contained agent Quigley ' s memorandum of the interview in the New Orleans jail on August 10th        O,O,O,O,O,O,O,O,O,O,LOC,LOC,O,O,O,O,PER,O,O,O,O,O,O,O,O,LOC,LOC,O,O,O,O\n",
       "472     12536.0                                               two of the agents returned to their rooms the seven others proceeded to an establishment called The Cellar Coffee House                                    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,ORG,ORG,O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(test_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df[6000:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>labels</th>\n",
       "      <th>token</th>\n",
       "      <th>label_asr</th>\n",
       "      <th>token_asr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>married</td>\n",
       "      <td>O</td>\n",
       "      <td>married</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8656</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8657</th>\n",
       "      <td>469</td>\n",
       "      <td>LOC</td>\n",
       "      <td>fort</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8658</th>\n",
       "      <td>469</td>\n",
       "      <td>LOC</td>\n",
       "      <td>worth</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8659</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>live</td>\n",
       "      <td>O</td>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8661</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>there</td>\n",
       "      <td>O</td>\n",
       "      <td>there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8662</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>until</td>\n",
       "      <td>O</td>\n",
       "      <td>until</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8663</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>coming</td>\n",
       "      <td>O</td>\n",
       "      <td>coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8664</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8665</th>\n",
       "      <td>469</td>\n",
       "      <td>LOC</td>\n",
       "      <td>new</td>\n",
       "      <td>LOC</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8666</th>\n",
       "      <td>469</td>\n",
       "      <td>LOC</td>\n",
       "      <td>orleans</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8667</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>he</td>\n",
       "      <td>O</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8668</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8669</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>told</td>\n",
       "      <td>O</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>469</td>\n",
       "      <td>LOC</td>\n",
       "      <td>new</td>\n",
       "      <td>LOC</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>469</td>\n",
       "      <td>LOC</td>\n",
       "      <td>orleans</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>arresting</td>\n",
       "      <td>O</td>\n",
       "      <td>arresting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>officers</td>\n",
       "      <td>O</td>\n",
       "      <td>officers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8675</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>that</td>\n",
       "      <td>O</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8676</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>he</td>\n",
       "      <td>O</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8677</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8678</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>been</td>\n",
       "      <td>O</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8679</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>born</td>\n",
       "      <td>O</td>\n",
       "      <td>born</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8680</th>\n",
       "      <td>469</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8681</th>\n",
       "      <td>469</td>\n",
       "      <td>LOC</td>\n",
       "      <td>cuba</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8682</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>470</td>\n",
       "      <td>ORG</td>\n",
       "      <td>fbi</td>\n",
       "      <td>ORG</td>\n",
       "      <td>FBI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8684</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>representative</td>\n",
       "      <td>O</td>\n",
       "      <td>representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8685</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8686</th>\n",
       "      <td>470</td>\n",
       "      <td>LOC</td>\n",
       "      <td>mexico</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8687</th>\n",
       "      <td>470</td>\n",
       "      <td>LOC</td>\n",
       "      <td>city</td>\n",
       "      <td>O</td>\n",
       "      <td>City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>arranged</td>\n",
       "      <td>O</td>\n",
       "      <td>arranged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>follow</td>\n",
       "      <td>O</td>\n",
       "      <td>follow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>up</td>\n",
       "      <td>O</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>this</td>\n",
       "      <td>O</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8693</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>information</td>\n",
       "      <td>O</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8694</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8695</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8696</th>\n",
       "      <td>470</td>\n",
       "      <td>ORG</td>\n",
       "      <td>cia</td>\n",
       "      <td>ORG</td>\n",
       "      <td>CIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8697</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8698</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8699</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>verify</td>\n",
       "      <td>O</td>\n",
       "      <td>verify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8700</th>\n",
       "      <td>470</td>\n",
       "      <td>PER</td>\n",
       "      <td>oswald</td>\n",
       "      <td>PER</td>\n",
       "      <td>Oswald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8701</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>entry</td>\n",
       "      <td>O</td>\n",
       "      <td>entry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8702</th>\n",
       "      <td>470</td>\n",
       "      <td>O</td>\n",
       "      <td>into</td>\n",
       "      <td>O</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8703</th>\n",
       "      <td>470</td>\n",
       "      <td>LOC</td>\n",
       "      <td>mexico</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8704</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>he</td>\n",
       "      <td>O</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8705</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "      <td>O</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8706</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>received</td>\n",
       "      <td>O</td>\n",
       "      <td>received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8707</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8708</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>copy</td>\n",
       "      <td>O</td>\n",
       "      <td>copy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8709</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8710</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8711</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>report</td>\n",
       "      <td>O</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>471</td>\n",
       "      <td>LOC</td>\n",
       "      <td>new</td>\n",
       "      <td>LOC</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8715</th>\n",
       "      <td>471</td>\n",
       "      <td>LOC</td>\n",
       "      <td>orleans</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>office</td>\n",
       "      <td>O</td>\n",
       "      <td>office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8717</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>which</td>\n",
       "      <td>O</td>\n",
       "      <td>which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>contained</td>\n",
       "      <td>O</td>\n",
       "      <td>contained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>agent</td>\n",
       "      <td>O</td>\n",
       "      <td>agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>471</td>\n",
       "      <td>PER</td>\n",
       "      <td>quigley</td>\n",
       "      <td>PER</td>\n",
       "      <td>Quigley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>'</td>\n",
       "      <td>O</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8722</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>s</td>\n",
       "      <td>O</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>memorandum</td>\n",
       "      <td>O</td>\n",
       "      <td>memorandum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8724</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8725</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>interview</td>\n",
       "      <td>O</td>\n",
       "      <td>interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>471</td>\n",
       "      <td>LOC</td>\n",
       "      <td>new</td>\n",
       "      <td>LOC</td>\n",
       "      <td>New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>471</td>\n",
       "      <td>LOC</td>\n",
       "      <td>orleans</td>\n",
       "      <td>LOC</td>\n",
       "      <td>Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>jail</td>\n",
       "      <td>O</td>\n",
       "      <td>jail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8732</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8733</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>august</td>\n",
       "      <td>O</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8734</th>\n",
       "      <td>471</td>\n",
       "      <td>O</td>\n",
       "      <td>10th</td>\n",
       "      <td>O</td>\n",
       "      <td>10th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8735</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>two</td>\n",
       "      <td>O</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8737</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8738</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>agents</td>\n",
       "      <td>O</td>\n",
       "      <td>agents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8739</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>returned</td>\n",
       "      <td>O</td>\n",
       "      <td>returned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8740</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8741</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>their</td>\n",
       "      <td>O</td>\n",
       "      <td>their</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8742</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>rooms</td>\n",
       "      <td>O</td>\n",
       "      <td>rooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8743</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8744</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>seven</td>\n",
       "      <td>O</td>\n",
       "      <td>seven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8745</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>others</td>\n",
       "      <td>O</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>proceeded</td>\n",
       "      <td>O</td>\n",
       "      <td>proceeded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8747</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8748</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>an</td>\n",
       "      <td>O</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8749</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>establishment</td>\n",
       "      <td>O</td>\n",
       "      <td>establishment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8750</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>called</td>\n",
       "      <td>O</td>\n",
       "      <td>called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8751</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8752</th>\n",
       "      <td>472</td>\n",
       "      <td>ORG</td>\n",
       "      <td>cellar</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Cellar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8753</th>\n",
       "      <td>472</td>\n",
       "      <td>ORG</td>\n",
       "      <td>coffee</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_no labels           token label_asr       token_asr\n",
       "8655         469      O         married         O         married\n",
       "8656         469      O              in         O              in\n",
       "8657         469    LOC            fort       LOC            Fort\n",
       "8658         469    LOC           worth       LOC           Worth\n",
       "8659         469      O             and         O             and\n",
       "8660         469      O            live         O            live\n",
       "8661         469      O           there         O           there\n",
       "8662         469      O           until         O           until\n",
       "8663         469      O          coming         O          coming\n",
       "8664         469      O              to         O              to\n",
       "8665         469    LOC             new       LOC             New\n",
       "8666         469    LOC         orleans       LOC         Orleans\n",
       "8667         469      O              he         O              he\n",
       "8668         469      O             had         O             had\n",
       "8669         469      O            told         O            told\n",
       "8670         469      O             the         O             the\n",
       "8671         469    LOC             new       LOC             New\n",
       "8672         469    LOC         orleans       LOC         Orleans\n",
       "8673         469      O       arresting         O       arresting\n",
       "8674         469      O        officers         O        officers\n",
       "8675         469      O            that         O            that\n",
       "8676         469      O              he         O              he\n",
       "8677         469      O             had         O             had\n",
       "8678         469      O            been         O            been\n",
       "8679         469      O            born         O            born\n",
       "8680         469      O              in         O              in\n",
       "8681         469    LOC            cuba       LOC            Cuba\n",
       "8682         470      O             the         O             the\n",
       "8683         470    ORG             fbi       ORG             FBI\n",
       "8684         470      O  representative         O  representative\n",
       "8685         470      O              in         O              in\n",
       "8686         470    LOC          mexico       LOC          Mexico\n",
       "8687         470    LOC            city         O            City\n",
       "8688         470      O        arranged         O        arranged\n",
       "8689         470      O              to         O              to\n",
       "8690         470      O          follow         O          follow\n",
       "8691         470      O              up         O              up\n",
       "8692         470      O            this         O            this\n",
       "8693         470      O     information         O     information\n",
       "8694         470      O            with         O            with\n",
       "8695         470      O             the         O             the\n",
       "8696         470    ORG             cia       ORG             CIA\n",
       "8697         470      O             and         O             and\n",
       "8698         470      O              to         O              to\n",
       "8699         470      O          verify         O          verify\n",
       "8700         470    PER          oswald       PER          Oswald\n",
       "8701         470      O           entry         O           entry\n",
       "8702         470      O            into         O            into\n",
       "8703         470    LOC          mexico       LOC          Mexico\n",
       "8704         471      O              he         O              he\n",
       "8705         471      O             had         O             had\n",
       "8706         471      O        received         O        received\n",
       "8707         471      O               a         O               a\n",
       "8708         471      O            copy         O            copy\n",
       "8709         471      O              of         O              of\n",
       "8710         471      O             the         O             the\n",
       "8711         471      O          report         O          report\n",
       "8712         471      O              of         O              of\n",
       "8713         471      O             the         O             the\n",
       "8714         471    LOC             new       LOC             New\n",
       "8715         471    LOC         orleans       LOC         Orleans\n",
       "8716         471      O          office         O          office\n",
       "8717         471      O           which         O           which\n",
       "8718         471      O       contained         O       contained\n",
       "8719         471      O           agent         O           agent\n",
       "8720         471    PER         quigley       PER         Quigley\n",
       "8721         471      O               '         O               '\n",
       "8722         471      O               s         O               s\n",
       "8723         471      O      memorandum         O      memorandum\n",
       "8724         471      O              of         O              of\n",
       "8725         471      O             the         O             the\n",
       "8726         471      O       interview         O       interview\n",
       "8727         471      O              in         O              in\n",
       "8728         471      O             the         O             the\n",
       "8729         471    LOC             new       LOC             New\n",
       "8730         471    LOC         orleans       LOC         Orleans\n",
       "8731         471      O            jail         O            jail\n",
       "8732         471      O              on         O              on\n",
       "8733         471      O          august         O          August\n",
       "8734         471      O            10th         O            10th\n",
       "8735         472      O             two         O             two\n",
       "8736         472      O              of         O              of\n",
       "8737         472      O             the         O             the\n",
       "8738         472      O          agents         O          agents\n",
       "8739         472      O        returned         O        returned\n",
       "8740         472      O              to         O              to\n",
       "8741         472      O           their         O           their\n",
       "8742         472      O           rooms         O           rooms\n",
       "8743         472      O             the         O             the\n",
       "8744         472      O           seven         O           seven\n",
       "8745         472      O          others         O          others\n",
       "8746         472      O       proceeded         O       proceeded\n",
       "8747         472      O              to         O              to\n",
       "8748         472      O              an         O              an\n",
       "8749         472      O   establishment         O   establishment\n",
       "8750         472      O          called         O          called\n",
       "8751         472      O             the         O             The\n",
       "8752         472    ORG          cellar       ORG          Cellar\n",
       "8753         472    ORG          coffee       ORG          Coffee\n",
       "8754         472      O           house         O           House"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label_asr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_tag</th>\n",
       "      <th>asr_tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, PER, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              model_tag                                                                     asr_tag\n",
       "sentence_no                                                                                                                                                        \n",
       "0            [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]\n",
       "1                       [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]\n",
       "10                                 [O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]                        [O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]\n",
       "100                      [O, O, O, O, O, O, O, O, O, PER, O, O, O, O, O, O, O, O, O, O]                [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
       "101             [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_tag</th>\n",
       "      <th>asr_tag</th>\n",
       "      <th>asr_sentence_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, O, O, LOC, ORG, LOC, ORG, O, O, O]</td>\n",
       "      <td>[O, O, O, LOC, O, LOC, LOC, O, O, O]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, LOC, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, LOC, O, O, O, O, O]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    model_tag                                                                     asr_tag  asr_sentence_no\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]                0\n",
       "1             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]                1\n",
       "2       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]                2\n",
       "3                                      [O, O, O, LOC, ORG, LOC, ORG, O, O, O]                                        [O, O, O, LOC, O, LOC, LOC, O, O, O]                3\n",
       "4                                      [O, O, O, O, O, O, LOC, O, O, O, O, O]                                      [O, O, O, O, O, O, LOC, O, O, O, O, O]                4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9610508280982296\n",
      "F1 Score:  0.8198347107438017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_analysis(test_df, original_data_path):\n",
    "    g_asr = test_df.groupby(\"sentence_no\")\n",
    "    asr_df = pd.DataFrame({'Sentence': g_asr.apply(lambda sdf: \" \".join(map(str,sdf.token))),\n",
    "                      'Tag': g_asr.apply(lambda sdf: \",\".join(sdf.labels))})\n",
    "    asr_df['asr_sentence_no'] = asr_df.index\n",
    "    asr_df[[\"asr_sentence_no\"]] = asr_df[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "    asr_df.sort_values('asr_sentence_no', inplace=True)\n",
    "    asr_df.reset_index(drop=True, inplace=True)\n",
    "    original = pd.read_csv(original_data_path)\n",
    "    original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    original = original[:7851]\n",
    "    g_original = original.groupby(\"Sentence #\")\n",
    "    original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    original_df.reset_index(inplace=True)\n",
    "    combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'].str.lower(),\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})\n",
    "    return asr_df, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_finding(tag, combined_df):\n",
    "#tag = \"PER\"\n",
    "    analysis = []\n",
    "    for i in range(0, len(combined_df), 1):\n",
    "        sample = combined_df.loc[[i]]\n",
    "        for original_sentence, asr_sentence, original_tag, asr_tag in zip(sample['original_sentence'].values.tolist(),\n",
    "                                                                          sample['asr_sentence'].values.tolist(),\n",
    "                                                                          sample['original_tags'].values.tolist(),\n",
    "                                                                          sample['asr_tags'].values.tolist()):\n",
    "            original_tag_token = np.array(original_tag.split(\",\"))\n",
    "            asr_tag_token = np.array(asr_tag.split(\",\"))\n",
    "            original_label = np.array(original_sentence.lower().split())\n",
    "            asr_label = np.array(asr_sentence.lower().split())\n",
    "\n",
    "            if tag in original_tag_token:\n",
    "                original_tag_ind = [index for index, element in enumerate(original_tag_token) if\n",
    "                                    original_tag_token[index] == tag]\n",
    "                if tag in asr_tag_token:\n",
    "                    asr_tag_ind = [index for index, element in enumerate(asr_tag_token) if\n",
    "                                       asr_tag_token[index] == tag]\n",
    "                    \n",
    "                    asr_tokens = []\n",
    "                    original_tokens = []\n",
    "                    errors = []\n",
    "                        # Sweynheim pannartz\n",
    "                        # Swain heim pannartz\n",
    "                    for ind in original_tag_ind:\n",
    "                        original_entity = original_label[ind]\n",
    "                        asr_entity = difflib.get_close_matches(original_entity, asr_label[asr_tag_ind])\n",
    "                        if len(asr_entity) > 0:\n",
    "                            asr_entity = asr_entity[0]\n",
    "                            error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(len(original_entity), len(asr_entity)))) * 100\n",
    "                            if error >= 50:\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(\"None\")\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        else:\n",
    "                            asr_tokens.append(\"None\")\n",
    "                            original_tokens.append(original_entity)\n",
    "                            errors.append(0.0)\n",
    "                    analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), True))\n",
    "                else:\n",
    "                    check = []\n",
    "                    o_label = original_label[original_tag_ind]\n",
    "                    for lab in o_label:\n",
    "                        j = 0\n",
    "                        for asr_lab in asr_label:\n",
    "                            local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                            if local_error >= 50.0:\n",
    "                                check.append(j)\n",
    "                            j = j + 1\n",
    "                    if len(check) > 0:\n",
    "                        asr_tokens = []\n",
    "                        original_tokens = []\n",
    "                        errors = []\n",
    "                        for ind in original_tag_ind:\n",
    "                            original_entity = original_label[ind]\n",
    "                            asr_entity = difflib.get_close_matches(original_entity, asr_label[check])\n",
    "                            if len(asr_entity) > 0:\n",
    "                                asr_entity = asr_entity[0]\n",
    "                                error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(\n",
    "                                len(original_entity), len(asr_entity)))) * 100\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(\"None\")\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), False))\n",
    "                    else:\n",
    "                        analysis.append((i, original_label[original_tag_ind], [\"None\"], [0.0], 0.0, False))\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein', 'Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([100.0,100.0]) == 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[orig_asr_found_complete['Sample #'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_nofound.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "#[45.56962025316456, 43.67088607594937, 9.49367088607595, 1.2658227848101267]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['original_sentence', 'original_tags']].to_csv(\"original_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[orig_asr_nofound['Sample #'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_org = ['chiswick press in 1844 revived castle on stops , printing for measures the diary of lady willoughby .',\n",
    " \"the position of our society that don ' t work of utility might be also a work of art , if we care to make it so .\",\n",
    " 'the chronicles of nougat , volume to . arthur griffiths . section for \" new gate down to 1818 .',\n",
    " \"full details of the arrangements are to be found in mr . neil ' s \\\" state of prisons in england , scotland , and wales , \\\" published in 1812 .\",\n",
    " 'alfred the great established the court baron , the hundred court , and the county court , which among other matters entertain please for debt .',\n",
    " \"lake county court was the sheriff ' s , who said they ' re surrounded by the bishop and the magnets of the county\",\n",
    " \"but as time passed , difficulties and delays in obtaining judgment led to the removal of causes to the great court of king ' s bench ,\",\n",
    " 'so much inconvenience ensued , that in 1518 the corporation obtained from parliament and act empowering to alderman',\n",
    " 'four common councilman to hold courts of requests , or courts of conscience , to hear and determine all causes of death',\n",
    " 'other cases are recorded elsewhere , that the gilts 1st street concert , where in 1805 mister . kneeled found a man named william grant',\n",
    " 'mr buxton . in his \" inquiry into the system of prison discipline , \"',\n",
    " 'to those in other london prisons , for new gate was not the only place of durance for these unfortunate people . there were also the kings bench ,',\n",
    " 'the fleet , and the marshalsea prison especially devoted to them ,',\n",
    " 'the sale of spirits was forbidden , but june could always be had at the whistling shops , where it was known as moonshine , sky blue ,',\n",
    " 'the fleet , which stood in farrington street ,',\n",
    " 'the warden of the fleet at the commencement of the 18th century , are too well known to need more than a passing reference .',\n",
    " 'the committee on jails reported , that quote , although the house of the warden looked into the court ,',\n",
    " 'and came under the strong animated version of the jail committee of 1729 ,',\n",
    " 'the lord steward of the household , the stewart and officers of the marshalsea court , and others .',\n",
    " 'comforters of ludgate , giltspur street , and the borough where discontinued as debtors \\' prisons ( as was newgate also )',\n",
    " \"clergyman , proctor ' s , attorneys , and persons specially selected by the corporation .\",\n",
    " 'at one time the ludgate debtors , accompanied by the keeper ,',\n",
    " \"spruce street compton received sheriff ' s debtors , also felons , vagrants , and knight charges .\",\n",
    " 'it was generally crowded , as debtors who would have gone to the poultry copter we sent to giltspur street when the former was condemned as unfit to receive prisoners .',\n",
    " \"the borough compter was in a disgraceful state to the last . the men ' s ward had an earth or rather a mud , floor ,\",\n",
    " \"notably as when numbers filled new gate in anticipation of lord reds dale ' s bill for insolvent debtors ,\",\n",
    " 'is gradually was forced upon the consciousness of the corporation ,',\n",
    " 'bypass now to the criminal side of newgate , which consisted of the six quarters or yards already enumerated and describe .',\n",
    " 'it was particularly recommended by the committee on jails in 1814',\n",
    " \"send mr . addison , keeper of new gate , to make a visitation of the jail ' s supposed to be the best managed , including those of petworth and gloucester ,\",\n",
    " 'the committee did not deny the superior advantages offered by such prisons as gloucester and petworth ,',\n",
    " 'the committee does not seem to have yet understood that new gate could be only and properly replaced',\n",
    " 'buy a new jail built on the outskirts , as holloway eventually was , and committed itself to be altogether counter',\n",
    " \"i ' m checked in its efforts towards reform by the prohibitory costliness of the land about nougat .\",\n",
    " 'why not relieving you gate more largely upon the superior accommodation which build bank offer ?',\n",
    " 'chronicles of new gate , volume 2 . by arthur griffith . section 7 : the beginnings of prison reform .',\n",
    " 'i have shown in a previous chapter what new gate was at this , despite a vast expenditure and boasted efforts to introduce reforms .',\n",
    " 'one of the moving spirits was the honorable h . g bennett , auntie , whose vigorous protests against the lamentable condition of newgate have already been recorded .',\n",
    " 'as the prison discipline society pertinently observed in a report dated 1820 ,',\n",
    " 'the chronicles of nougat , volume 2 . by arthur griffith . section 8 : the beginnings of prison reform .',\n",
    " 'newgate prisoners were the victims to another most objectionable practice which obtained all over london .',\n",
    " 'the society for the improvement of prison discipline was taxed with a desire to introduce a system',\n",
    " 'an imputation which the society indignantly and very justly repudiated , the statement being , as they said ,',\n",
    " 'among those from the society found a raid against it was sidney smith ,',\n",
    " 'admitting the good intentions of the society , he condemned there ultra humanitarianism as misplaced ,',\n",
    " 'he took exception to various of the proposals of the society . he thought they linked too much to a system of indulgences and education in jails .',\n",
    " \"society pursuit it ' s laudable undertaking with a remarkable energy and great singleness of purpose .\",\n",
    " 'a much - needed and , according to our ideas , indispensable reform , already initiated by the ladies \\' committee at newgate .',\n",
    " 'brandon moral and religious duty , and which after a time sought to provide them with suitable situations , was supported entirely out of the funds of the society ,',\n",
    " 'another point to which the society devoted infinite was the preparation of plans for the guidance of architects in the construction of prisons .',\n",
    " 'a very valuable volume published by the society',\n",
    " 'was introduced as early as 1790 by mr . blackburn',\n",
    " 'the society did not limit its remarks to the description of what had already been done',\n",
    " 'the prison society reproves the misdirected efforts of ambitious architect , to buy a lavish an improvident expenditure of public money',\n",
    " \"these are principles fully recognized now - a - days , and it may fairly be conceded that the prison discipline society ' s ideal\",\n",
    " 'after a few years of active exertion the society was rewarded by fresh legislation .',\n",
    " 'to its efforts , and their effect upon parliament and the public mind , we must attribute the new jail acts of for george the 4th',\n",
    " 'the promulgation of these to jail acts strengthen the hands of the prison discipline society enormously .',\n",
    " 'the society did not shrink from its self - imposed duty , but continued year after year , with unflagging energy and unflinching spirit , to watch closely',\n",
    " 'upon these and the private visitations made by various members of the society obtained effects ,',\n",
    " 'four years later the prison society reported',\n",
    " 'i just chillin by the report of the commissioners to inquire into the state of the municipal corporations in 1835 .',\n",
    " 'kidderminster had a prison , one dance chill room ,',\n",
    " 'in 1827 the society was compelled to report that \" no material change has taken place in newgate since the passing of the prison laws ,',\n",
    " 'the prison society did not relax its efforts as time passed , but its leading members had other and more pressing claims upon their energies .',\n",
    " 'this committee anniversary strongly upon the system in force at the metropolitan jails , and more especially upon the condition of nougat',\n",
    " 'mister . samuel hoare was examined by this committee',\n",
    " 'i stated that in his opinion new gate , as the common jail of middlesex , was wholly inadequate to the proper confinement of its prisoners .',\n",
    " 'the committee was appointed , under the presidency of the duke of richmond',\n",
    " 'the whole question was again dealt with in lord john russell bill for the reform of the municipal corporations , and with a more liberal election of town councillors ,',\n",
    " 'newgate has remained rather in the background while the whole of the jails as a body wear under discussion .',\n",
    " 'william crawford had been one of the promoters and managers of the philanthropic societies farm school .',\n",
    " 'unlimited it to newgate alone . newgate indeed became the sole seam of their first report .',\n",
    " 'it was no longer the faintest possible excuse for overcrowding . the numbers now committed to newgate',\n",
    " 'temporary fences of the most varying description by the central criminal court .',\n",
    " 'but incredible as it may appear , the authorities of newgate declined to avail themselves of the advantages offered them ,',\n",
    " 'famous female side , where the ladies \\' association still reigned supreme , more system and a greater semblance of decorum was maintained .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loc = ['printed very few books in this type 3 only but in their very first book syndrome , beginning with the year 1468 ,',\n",
    " 'the chronicles of nougat , volume to . arthur griffiths . section for : new gate down to 1818 .',\n",
    " 'seldom let a session go by without visiting you gate .',\n",
    " 'returns laid before the house of commons showed that 6439 persons have been committed to nougat',\n",
    " 'there was in the city road a temporary bar , with a collector of tolls who was sometimes on the spot and sometimes not .',\n",
    " 'the best , or at least the most influential prisoners , god lodging in the statehouse , which contained \" eight large handsome rooms . \"',\n",
    " 'in consequence of these disclosures , bambridge and hugging , his predecessor in the office , or committed to newgate ,',\n",
    " 'senators were rather better at the marshalsea ,',\n",
    " 'is bequest , which was charged upon his manner at goering , auxins , and hence called the oxford charity ,',\n",
    " 'the chronicles of nougat , volume 2 . by arthur griffiths . section v : newgate down to 18 18 , part 2 .',\n",
    " 'notes for street , and the poultry , or about 476 and all .',\n",
    " 'such as measures . virtual corn hill and messrs . leach and dollemore of ludgate hill .',\n",
    " 'it was very desirable that there should be a more speedy removal of transports from you gate to the ships .',\n",
    " 'specify more particularly one or two of the worst , it may be mentioned that in the boro comforter',\n",
    " 'the system not adopted generally till nearly half a century later had already prevail that bill chester .',\n",
    " 'for the reception of deserving cases discharged from prison . the governor a new gate and other metropolitan prisons had orders of admission to this refuge',\n",
    " 'this committee anniversary strongly upon the system in force at the metropolitan jails , and more especially upon the condition of nougat',\n",
    " 'he blamed the construction of new gate for the neglect of classification , and was yet compelled to confess that he had made no attempt whatever to carry it out .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_per = ['especially as regards to lower - case letters and type very similar was used during the next 15 or 20 years not only by chauffeur ,',\n",
    " 'about the same year mental in at strasburg began to print in a type which is distinctly roman',\n",
    " 'and though the famous family of aldis restored its technical excellence , rejecting battered letters ,',\n",
    " 'most of caxton zone types of an earlier character ,',\n",
    " 'now come into general use that are obviously a great improvement on the ordinary \" modern style \" and use in england , which is in fact the bodony type',\n",
    " 'on the top of the jail , continues neeld arawatch - house and a century - box where two or more guards , with dogs and firearms ,',\n",
    " 'these courts were extended to centuries later to several large provincial towns , and all were in full activity when nield road ,',\n",
    " \"he had been in the employ of a corn - chandler at islington , and went into london with his master ' s cart and horse .\",\n",
    " \"neil ' s gives a list of the various items charged upon a debt of 10 , lb which included instructions to sue ,\",\n",
    " 'shameful malpractices of bambridge ,',\n",
    " 'the lord steward of the household , the stewart and officers of the marshalsea court , and others .',\n",
    " 'if they happened to be in funds - - among whom was the marquis of slego in 1811 .',\n",
    " 'mister . kneeled , a second howard ,',\n",
    " 'the most important jail active satterlee , however , was the 24 george the third . c . 54 , s . 4 ( 1784 )',\n",
    " 'classification was insisted upon , in the manner laid down by the 24 george the third . cat . 54 ,',\n",
    " 'which became the four george the 4th . tap . 64 , said that he had abstained from legislating for these small jurisdictions \" on mature deliberation . \"',\n",
    " 'nothing was more prominently brought out by the inspectors and the inefficiency of the governor at that time , mister . co .',\n",
    " 'watkins \\' disney - joint is very severely injured']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_analysis(sample_df, combined_df):\n",
    "    ind = np.array(sample_df['Sample #'].values.tolist())\n",
    "    df = combined_df.loc[ind]\n",
    "    df.insert(2,'Original',sample_df['Original'].values.tolist())\n",
    "    df.insert(5,'ASR',sample_df['ASR'].values.tolist())\n",
    "    df.drop(['original_tags', 'asr_tags'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern = pattern_analysis(orig_asr_similar, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling(df):\n",
    "    i = 0\n",
    "    equal_length_samples = []\n",
    "    variable_length_samples = []\n",
    "    for sample, original, asr in zip(df.index, \n",
    "                                     df['Original'],\n",
    "                                     df['ASR']):\n",
    "        if len(original) == len(asr):\n",
    "            equal_length_samples.append(sample)\n",
    "        else:\n",
    "            variable_length_samples.append(sample)\n",
    "    equal_length_samples.sort()\n",
    "    variable_length_samples.sort()\n",
    "    equal_length_samples_df = df.loc[equal_length_samples]\n",
    "    variable_length_samples_df = df.loc[variable_length_samples]\n",
    "    return equal_length_samples_df, variable_length_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df, variable_length_words_samples_df = error_sampling(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variable_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variable_length_words_samples_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_words_simulation(sampled_df):\n",
    "    simulated_asr = []\n",
    "    for sample, original_sentence, asr_sentence, original, asr in zip(sampled_df.index,\n",
    "                                     sampled_df['original_sentence'],\n",
    "                                     sampled_df['asr_sentence'],\n",
    "                                     sampled_df['Original'],\n",
    "                                     sampled_df['ASR']):\n",
    "\n",
    "        for x,y in zip(original, asr):\n",
    "            #original_words.append(x)\n",
    "            #asr_words.append(y)\n",
    "            if y in asr_sentence:\n",
    "                asr_sentence = asr_sentence.replace(y, x)\n",
    "            \n",
    "        simulated_asr.append((sample, asr_sentence))\n",
    "    simulated_asr_df = pd.DataFrame(simulated_asr)\n",
    "    return simulated_asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_words_simulation(df):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "            df.index,\n",
    "            df['original_sentence'].values.tolist(),\n",
    "            df['asr_sentence'].values.tolist(),\n",
    "            df['Original'].values.tolist(),\n",
    "            df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        o_label = original_label[original_tag_ind]\n",
    "        for lab in original_tag:\n",
    "            for asr_lab in asr_tag:\n",
    "                local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                if local_error >= 50.0:\n",
    "                    asr_sentence = asr_sentence.replace(asr_lab, lab)\n",
    "        check.append((sample, asr_sentence))\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df(asr_df, simulated_df):\n",
    "    asr_df.loc[simulated_df[0].values.tolist(), 'Sentence'] = simulated_df[1].values.tolist()\n",
    "    return asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = equal_words_simulation(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.loc[simulated_asr_df[0].values.tolist(), 'Sentence'] = test_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulated_asr_df = variable_words_simulation(variable_length_words_samples_df)\n",
    "#simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = test_df[test_df['token'] == \"[CLS]\" ].index\n",
    "test_df.drop(indexNames, inplace=True)\n",
    "indexNames = test_df[test_df['token'] == \"[SEP]\" ].index\n",
    "test_df.drop(indexNames, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = prepare_model_output(test_df, new_df)\n",
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_similar.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_simulated_df = combined_df.loc[orig_asr_similar['Sample #'].values.tolist(),['original_sentence','original_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_simulated_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = df.groupby(\"Sentence #\")\n",
    "x = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no = list(range(0, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.index = sentence_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[context_simulated_df.index.tolist(), 'Sentence'] = context_simulated_df['original_sentence'].values.tolist()\n",
    "x.loc[context_simulated_df.index.tolist(), 'Tag'] = context_simulated_df['original_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_simulated_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.loc[context_simulated_df.index.tolist(), 'Sentence'] = context_simulated_df['original_sentence'].values.tolist()\n",
    "asr_df.loc[context_simulated_df.index.tolist(), 'Tag'] = context_simulated_df['original_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no = 0\n",
    "dataset=[]\n",
    "for sentences, tags in zip(x['Sentence'].values.tolist(), x['Tag'].values.tolist()):\n",
    "    sentence=sentences.split(\" \")\n",
    "    tag = tags.split(\",\")\n",
    "    for word, label in zip(sentence, tag):\n",
    "        dataset.append((sentence_no, word, label))\n",
    "    sentence_no = sentence_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(dataset, columns=['Sentence #', 'Word', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('unprocessed_sampled_original.csv')\n",
    "original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "original = original[:7851]\n",
    "g_original = original.groupby(\"Sentence #\")\n",
    "original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "original_df.reset_index(inplace=True)\n",
    "combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'],\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_context(df, n_grams):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "                df.index,\n",
    "                df['original_sentence'].values.tolist(),\n",
    "                df['asr_sentence'].values.tolist(),\n",
    "                df['Original'].values.tolist(),\n",
    "                df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        for l in original_tag_ind:\n",
    "            if l <= (len(original_label)-1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams+1, 1):\n",
    "                    if l+c >= 0:\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l+c < len(original_label):\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "        for l in asr_tag_ind:\n",
    "            if l <= (len(asr_label) - 1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams + 1, 1):\n",
    "                    if l + c >= 0:\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l + c < len(asr_label):\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "        \n",
    "        check.append((sample, original_bigrams[0], original_sentence, original_tag, asr_bigrams[0], asr_sentence, asr_tag))\n",
    "    context = pd.DataFrame(check)\n",
    "    context.columns = ['Sample #', 'Original N-Grams', \"original_sentence\", \"Original\", \"ASR N-Grams\", \"asr_sentence\", \"ASR\"]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling3(context):\n",
    "    check = []\n",
    "    for sample, original_ngrams, original_sentence, asr_ngrams, asr_sentence, original_tag, asr_tag in zip(\n",
    "            context['Sample #'].values.tolist(),\n",
    "            context['Original N-Grams'].values.tolist(),\n",
    "            context['original_sentence'].values.tolist(),\n",
    "            context['ASR N-Grams'].values.tolist(),\n",
    "            context['asr_sentence'].values.tolist(),\n",
    "            context['Original'].values.tolist(),\n",
    "            context['ASR'].values.tolist()):\n",
    "        \n",
    "        original_ngrams = np.array(original_ngrams.split(\" \"))\n",
    "        asr_ngrams = np.array(asr_ngrams.split(\" \"))\n",
    "        \n",
    "        local_errors = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for _original in original_tag:\n",
    "            if _original in asr_tag:\n",
    "                if len(asr_ngrams) < len(asr_tag):\n",
    "                    continue\n",
    "                \n",
    "                print(asr_sentence)\n",
    "                asr_sentence = asr_sentence.replace(\"\".join(asr_ngrams[i].rstrip()), \"\".join(original_ngrams[i].rstrip()))\n",
    "                print(asr_sentence)\n",
    "                #print(check)\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "            else:\n",
    "                j = j + 1\n",
    "        check.append((sample, asr_sentence))\n",
    "        print(\"---------------\")\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = finding_context(equal_length_words_samples_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = error_sampling3(context)\n",
    "simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context = finding_context(variable_length_words_samples_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulated_asr_df = error_sampling3(context)\n",
    "#simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"and though the famous family of aldus restored its technical excellence , rejecting battered letters ,\",\"most of caxton ' s zone types of an earlier character\", \"are the leaders in this luckless change , though our own baskerville , who was at work some years before them , went much on the same lines\",\n",
    "       \"now come into general use that are obviously a great improvement on the ordinary \\\" modern style \\\" and use in england , which is in fact the bodoni type\" , \"on the top of the jail , continues neild , arawatch - house and a century - box , where two or more guards , with dogs and firearms ,\" ,\n",
    "       \"these courts were extended to centuries later to several large provincial towns , and all were in full activity when neild road ,\" , \"he had been in the employ of a corn - chandler at islington , and went into london with his master ' s cart and horse .\",\n",
    "       \"shameful malpractices of bambridge ,\" , \"if they happened to be in funds - - among whom was the marquis of slego in 1811\", \n",
    "       \"mister . neild , a second howard ,\", \"again the 22 charles ii . c20 order the jailer to keep felons and debtors \\\" separate and apart from one another ,\",\n",
    "       \"prisoners were crowded together in the jail , contrary to the requirements of the for george the 4th .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"have now come into general use and are obviously a great improvement on the ordinary \\\" modern style \\\" in use in england , which is in fact the bodoni type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = model_test([test], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
