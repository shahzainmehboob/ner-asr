{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('max_colwidth', 10000)\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "import Levenshtein\n",
    "import string\n",
    "import difflib\n",
    "\n",
    "transformers.__version__\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tag_values = ['O', 'PER', 'LOC', 'ORG']\n",
    "#tag_values = ['B-ORG', 'O', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_whole_word_mask=True)\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"../model_2/bert_base_conll_50.pt\", map_location=torch.device('cpu'))) \n",
    "model.load_state_dict(torch.load(\"../model/with_punctuation_with_all_broken_entities_vertical_concatenation_100_2.pt\", map_location=torch.device('cpu'))) # WITH PHONETIC NOISE\n",
    "#model.load_state_dict(torch.load(\"../model_2/bert_base_conll_with_punctuation_with_broken_entities_75.pt\", map_location=torch.device('cpu'))) # WITH BROKEN ENTITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_test(filepath):\n",
    "    df = pd.read_csv(filepath, sep=\";\")\n",
    "    #df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    #df = df[:6723]\n",
    "    g_test = df.groupby(\"Sentence #\")\n",
    "    test_df = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    test_df.reset_index(inplace=True)\n",
    "    return df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(data, tokenizer, model):\n",
    "    test = []\n",
    "    #results = open(\"conll03_base_ljspeech_asr_test_without_gpe_uncased_results_lower.txt\", \"a+\")\n",
    "    #test_data=original_data['sentence'].values.tolist()\n",
    "    #test_data=original_sentence\n",
    "    #test_data=test_df['Sentence'].values.tolist()\n",
    "    test_data=data\n",
    "\n",
    "    # ASR TEST DATE LATEST\n",
    "    sentence_no = 0\n",
    "    for data in test_data:\n",
    "        tokenized_sentence = tokenizer.encode(data.lower().strip())\n",
    "        #tokenized_sentence = nlp(data.lower().strip())\n",
    "        input_ids = torch.tensor([tokenized_sentence])\n",
    "        #input_ids = torch.tensor([tokenized_sentence._.trf_word_pieces])\n",
    "\n",
    "        with torch.no_grad():\n",
    "             output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        #tokens = _.trf_word_pieces_\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(tag_values[label_idx])\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            #result = str(sentence_no) + \"\\t\" + label + \"\\t\" + token + \"\\n\"\n",
    "            #results.write(result)\n",
    "            test.append((str(sentence_no), label, token))\n",
    "        sentence_no = sentence_no + 1\n",
    "    test_df = pd.DataFrame(test, columns=['sentence_no', 'labels', 'token'])\n",
    "    return test_df\n",
    "    #test_df.to_csv(\"final_asr_test_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_output(test_df, df):\n",
    "    indexNames = test_df[test_df['token'] == \"[CLS]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    indexNames = test_df[test_df['token'] == \"[SEP]\" ].index\n",
    "    test_df.drop(indexNames, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "    test_df['label_asr'] = df['Tag']\n",
    "    test_df['token_asr'] = df['Word']\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(test_df, tags):\n",
    "    new_acc = accuracy_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_acc)\n",
    "\n",
    "    new_f1 = f1_score(test_df['labels'].values.tolist(), test_df['label_asr'].values.tolist())\n",
    "    print(new_f1)\n",
    "    print(\"---STATISTICS ON EACH LABEL---\")\n",
    "    for tag in tags:\n",
    "        true_positive = test_df[((test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_positive))\n",
    "        false_positive = test_df[((test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_positive))\n",
    "        false_negative = test_df[((~test_df['labels'].str.contains(tag)) & (test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(false_negative))\n",
    "        true_negative = test_df[((~test_df['labels'].str.contains(tag)) & (~test_df['label_asr'].str.contains(tag)))]\n",
    "        print(len(true_negative))\n",
    "        prec = len(true_positive) / (len(true_positive) + len(false_positive))\n",
    "        print(prec)\n",
    "        recall = len(true_positive) / (len(true_positive) + len(false_negative))\n",
    "        print(recall)\n",
    "        f_measure = (2 * prec * recall) / (prec + recall)\n",
    "        print(f_measure)\n",
    "        print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, test_df = prepare_data_for_test('unprocessed_sampled_asr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>for although the Chinese took Impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands by a similar process</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>what the first Bible actually dated which also was printed at mace by Peter Shaffer in the year 1462</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,LOC,O,PER,PER,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>especially as regards to lowercase letters and type very similar was used during the next 15 or 20 years not only by chauffeur</td>\n",
       "      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.0</td>\n",
       "      <td>Buy printers in Strasburg basil Paris Lubec and other cities</td>\n",
       "      <td>O,O,O,LOC,O,LOC,LOC,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>but don ' t expect in Italy letter with most often used</td>\n",
       "      <td>O,O,O,O,O,O,LOC,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #                                                                                                                                                    Sentence                                                Tag\n",
       "0         2.0  for although the Chinese took Impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands by a similar process  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,LOC,O,O,O,O\n",
       "1        23.0                                                        what the first Bible actually dated which also was printed at mace by Peter Shaffer in the year 1462        O,O,O,O,O,O,O,O,O,O,O,LOC,O,PER,PER,O,O,O,O\n",
       "2        26.0                              especially as regards to lowercase letters and type very similar was used during the next 15 or 20 years not only by chauffeur    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,PER\n",
       "3        27.0                                                                                                Buy printers in Strasburg basil Paris Lubec and other cities                          O,O,O,LOC,O,LOC,LOC,O,O,O\n",
       "4        28.0                                                                                                     but don ' t expect in Italy letter with most often used                          O,O,O,O,O,O,LOC,O,O,O,O,O"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(test_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_no</th>\n",
       "      <th>labels</th>\n",
       "      <th>token</th>\n",
       "      <th>label_asr</th>\n",
       "      <th>token_asr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8749</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>called</td>\n",
       "      <td>O</td>\n",
       "      <td>called</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8750</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8751</th>\n",
       "      <td>472</td>\n",
       "      <td>ORG</td>\n",
       "      <td>cellar</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Cellar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8752</th>\n",
       "      <td>472</td>\n",
       "      <td>ORG</td>\n",
       "      <td>coffee</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8753</th>\n",
       "      <td>472</td>\n",
       "      <td>O</td>\n",
       "      <td>house</td>\n",
       "      <td>O</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_no labels   token label_asr token_asr\n",
       "8749         472      O  called         O    called\n",
       "8750         472      O     the         O       The\n",
       "8751         472    ORG  cellar       ORG    Cellar\n",
       "8752         472    ORG  coffee       ORG    Coffee\n",
       "8753         472      O   house         O     House"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()\n",
    "output['ground_truth_token'] = test_df['token']\n",
    "output['ground_truth_label'] = test_df['label_asr']\n",
    "output['model_label'] = test_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth_token</th>\n",
       "      <th>ground_truth_label</th>\n",
       "      <th>model_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>although</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chinese</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>took</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ground_truth_token ground_truth_label model_label\n",
       "0                for                  O           O\n",
       "1           although                  O           O\n",
       "2                the                  O           O\n",
       "3            chinese                  O           O\n",
       "4               took                  O           O"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('with_punctuation_with_all_broken_entities_vertical_concatenation_100_2', sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'LOC', 'PER', 'ORG'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label_asr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_tag</th>\n",
       "      <th>asr_tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, PER, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              model_tag                                                                     asr_tag\n",
       "sentence_no                                                                                                                                                        \n",
       "0            [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]\n",
       "1                       [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]\n",
       "10                                 [O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]                        [O, O, LOC, O, O, O, O, O, O, O, O, LOC, O, O, O, O]\n",
       "100                      [O, O, O, O, O, O, O, O, O, PER, O, O, O, O, O, O, O, O, O, O]                [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]\n",
       "101             [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER, O, O, O, O]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_tag</th>\n",
       "      <th>asr_tag</th>\n",
       "      <th>asr_sentence_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[O, O, O, LOC, ORG, LOC, ORG, O, O, O]</td>\n",
       "      <td>[O, O, O, LOC, O, LOC, LOC, O, O, O]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[O, O, O, O, O, O, LOC, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, LOC, O, O, O, O, O]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    model_tag                                                                     asr_tag  asr_sentence_no\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, LOC, O, O, O, O]                0\n",
       "1             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]             [O, O, O, O, O, O, O, O, O, O, O, LOC, O, PER, PER, O, O, O, O]                1\n",
       "2       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, PER]                2\n",
       "3                                      [O, O, O, LOC, ORG, LOC, ORG, O, O, O]                                        [O, O, O, LOC, O, LOC, LOC, O, O, O]                3\n",
       "4                                      [O, O, O, O, O, O, LOC, O, O, O, O, O]                                      [O, O, O, O, O, O, LOC, O, O, O, O, O]                4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9710989262051634\n",
      "F1 Score:  0.8881469115191986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PER seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/Users/shahzainmehboob/opt/anaconda3/envs/thesis/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ORG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_analysis(test_df, original_data_path):\n",
    "    g_asr = test_df.groupby(\"sentence_no\")\n",
    "    asr_df = pd.DataFrame({'Sentence': g_asr.apply(lambda sdf: \" \".join(map(str,sdf.token))),\n",
    "                      'Tag': g_asr.apply(lambda sdf: \",\".join(sdf.labels))})\n",
    "    asr_df['asr_sentence_no'] = asr_df.index\n",
    "    asr_df[[\"asr_sentence_no\"]] = asr_df[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "    asr_df.sort_values('asr_sentence_no', inplace=True)\n",
    "    asr_df.reset_index(drop=True, inplace=True)\n",
    "    original = pd.read_csv(original_data_path, sep=\";\")\n",
    "    #original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    #original = original[:6723]\n",
    "    g_original = original.groupby(\"Sentence #\")\n",
    "    original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "    original_df.reset_index(inplace=True)\n",
    "    combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'].str.lower(),\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})\n",
    "    return asr_df, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_finding(tag, combined_df):\n",
    "#tag = \"PER\"\n",
    "    analysis = []\n",
    "    for i in range(0, len(combined_df), 1):\n",
    "        sample = combined_df.loc[[i]]\n",
    "        for original_sentence, asr_sentence, original_tag, asr_tag in zip(sample['original_sentence'].values.tolist(),\n",
    "                                                                          sample['asr_sentence'].values.tolist(),\n",
    "                                                                          sample['original_tags'].values.tolist(),\n",
    "                                                                          sample['asr_tags'].values.tolist()):\n",
    "            original_tag_token = np.array(original_tag.split(\",\"))\n",
    "            asr_tag_token = np.array(asr_tag.split(\",\"))\n",
    "            original_label = np.array(original_sentence.lower().split())\n",
    "            asr_label = np.array(asr_sentence.lower().split())\n",
    "\n",
    "            if tag in original_tag_token:\n",
    "                original_tag_ind = [index for index, element in enumerate(original_tag_token) if\n",
    "                                    original_tag_token[index] == tag]\n",
    "                if tag in asr_tag_token:\n",
    "                    asr_tag_ind = [index for index, element in enumerate(asr_tag_token) if\n",
    "                                       asr_tag_token[index] == tag]\n",
    "                    \n",
    "                    asr_tokens = []\n",
    "                    original_tokens = []\n",
    "                    errors = []\n",
    "                        # Sweynheim pannartz\n",
    "                        # Swain heim pannartz\n",
    "                    for ind in original_tag_ind:\n",
    "                        original_entity = original_label[ind]\n",
    "                        asr_entity = difflib.get_close_matches(original_entity, asr_label[asr_tag_ind])\n",
    "                        if len(asr_entity) > 0:\n",
    "                            asr_entity = asr_entity[0]\n",
    "                            error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(len(original_entity), len(asr_entity)))) * 100\n",
    "                            if error >= 50:\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(asr_label[ind])\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        else:\n",
    "                            asr_tokens.append(asr_label[ind])\n",
    "                            original_tokens.append(original_entity)\n",
    "                            errors.append(0.0)\n",
    "                    analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), True))\n",
    "                else:\n",
    "                    check = []\n",
    "                    o_label = original_label[original_tag_ind]\n",
    "                    for lab in o_label:\n",
    "                        j = 0\n",
    "                        for asr_lab in asr_label:\n",
    "                            local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                            if local_error >= 50.0:\n",
    "                                check.append(j)\n",
    "                            j = j + 1\n",
    "                    if len(check) > 0:\n",
    "                        asr_tokens = []\n",
    "                        original_tokens = []\n",
    "                        errors = []\n",
    "                        for ind in original_tag_ind:\n",
    "                            original_entity = original_label[ind]\n",
    "                            asr_entity = difflib.get_close_matches(original_entity, asr_label[check])\n",
    "                            if len(asr_entity) > 0:\n",
    "                                asr_entity = asr_entity[0]\n",
    "                                error = (1 - (Levenshtein.distance(original_entity, asr_entity) / max(\n",
    "                                len(original_entity), len(asr_entity)))) * 100\n",
    "                                asr_tokens.append(asr_entity)\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(error)\n",
    "                            else:\n",
    "                                asr_tokens.append(asr_label[ind])\n",
    "                                original_tokens.append(original_entity)\n",
    "                                errors.append(0.0)\n",
    "                        analysis.append((i, original_tokens, asr_tokens, errors, np.mean(errors), False))\n",
    "                    else:\n",
    "                        analysis.append((i, original_label[original_tag_ind], [\"None\"], [0.0], 0.0, False))\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_asr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "sample = []\n",
    "for sample_no, original_tag, asr_tag in zip(combined_df.index, combined_df['original_tags'].values.tolist(), combined_df['asr_tags'].values.tolist()):\n",
    "    x_tag = original_tag.split(\",\")\n",
    "    y_tag = asr_tag.split(\",\")\n",
    "    for x,y in zip(x_tag, y_tag):\n",
    "        if x == \"ORG\" and y == \"O\":\n",
    "            sample.append(sample_no)\n",
    "print(sample)\n",
    "\n",
    "# ================================================== BASELINE ================================================\n",
    "\n",
    "# [1, 1, 6, 6, 6, 9, 14, 14, 15, 15, 18, 20, 21, 21, 21, 21, 27, 27, 29, 30, 30, 33, 35, 37, 38, 38, 38, 45, 46, 47, 50, 50, 52, 53, 55, 62, 63, 66, 66, 70, 72, 72, 72, 78, 80, 81, 81, 82, 82, 83, 83, 84, 84, 85, 85, 86, 87, 87, 89, 93, 101, 106, 109, 110, 110, 110, 111, 118, 126, 138, 139, 142, 143, 145, 145, 146, 146, 159, 159, 160, 161, 163, 164, 165, 170, 171, 172, 173, 174, 174, 176, 177, 178, 179, 180, 181, 181, 182, 182, 187, 193, 204, 204, 205, 206, 208, 208, 211, 211, 212, 212, 212, 212, 212, 212, 213, 213, 214, 214, 214, 215, 215, 215, 215, 216, 216, 218, 219, 219, 220, 221, 222, 223, 224, 225, 226, 227, 232, 233, 234, 237, 237, 238, 239, 244, 244, 246, 246, 247, 248, 250, 255, 255, 262, 262, 272, 273, 275, 279, 280, 292, 298, 312, 312, 312, 315, 315, 317, 325, 325, 326, 326, 330, 330, 331, 331, 333, 333, 333, 333, 334, 334, 338, 338, 338, 339, 340, 341, 342, 342, 343, 343, 351, 352, 352, 357, 359, 359, 359, 363, 363, 365, 366, 366, 368, 369, 370, 373, 378, 378, 379, 380, 380, 383, 386, 391, 391, 393, 398, 402, 404, 404, 417, 420, 423, 425, 429, 430, 430, 431, 432, 434, 436, 444, 446, 446, 447, 449, 449, 449, 451, 454, 455, 455, 455, 460, 462, 465, 465, 470, 471] PER -> PER\n",
    "# [17, 31, 77, 199, 215, 269, 383, 397, 399, 430] PER -> LOC\n",
    "# [39, 65, 94, 107, 251, 363, 367, 367] PER -> ORG\n",
    "# [2, 67, 71, 84, 91, 125, 125, 125, 207, 207, 213, 215, 228, 281, 293, 399, 399, 400, 417] PER -> O\n",
    "\n",
    "\n",
    "\n",
    "# [0, 1, 3, 3, 3, 4, 5, 5, 5, 6, 6, 8, 9, 10, 10, 12, 13, 16, 19, 22, 22, 24, 26, 28, 30, 32, 32, 33, 33, 34, 37, 39, 40, 40, 41, 41, 42, 43, 48, 51, 51, 54, 56, 60, 60, 60, 60, 61, 62, 62, 62, 66, 68, 68, 68, 69, 69, 71, 79, 80, 84, 90, 90, 91, 91, 95, 96, 96, 96, 97, 103, 104, 104, 114, 114, 115, 115, 115, 117, 118, 123, 124, 126, 128, 129, 129, 129, 129, 130, 130, 131, 131, 134, 134, 134, 135, 135, 136, 136, 136, 139, 139, 140, 141, 144, 147, 150, 150, 150, 150, 150, 151, 151, 153, 154, 154, 154, 156, 159, 159, 162, 165, 166, 168, 169, 172, 175, 183, 183, 184, 185, 186, 189, 189, 193, 193, 193, 193, 196, 196, 197, 197, 197, 202, 205, 210, 213, 217, 230, 230, 234, 235, 235, 236, 240, 241, 241, 241, 242, 243, 246, 247, 249, 253, 256, 257, 259, 266, 267, 267, 267, 286, 288, 290, 290, 291, 294, 298, 299, 300, 300, 300, 301, 303, 303, 305, 306, 307, 307, 308, 309, 316, 319, 320, 320, 323, 323, 323, 328, 332, 335, 335, 336, 337, 338, 339, 341, 354, 358, 359, 360, 361, 372, 376, 377, 377, 379, 379, 379, 379, 380, 380, 380, 380, 380, 381, 381, 381, 381, 381, 382, 382, 382, 382, 383, 383, 384, 384, 384, 384, 385, 385, 385, 386, 386, 386, 388, 388, 388, 388, 389, 389, 389, 389, 390, 391, 391, 392, 392, 392, 392, 392, 393, 393, 393, 393, 393, 394, 394, 394, 395, 395, 395, 396, 396, 397, 397, 397, 398, 398, 398, 399, 400, 400, 400, 401, 403, 403, 403, 403, 404, 404, 404, 404, 405, 405, 406, 406, 406, 406, 406, 406, 406, 407, 407, 408, 408, 408, 408, 409, 410, 410, 410, 410, 410, 410, 411, 411, 411, 411, 412, 412, 412, 413, 413, 413, 413, 413, 414, 414, 414, 414, 414, 415, 415, 415, 415, 415, 415, 415, 416, 416, 416, 416, 416, 416, 416, 417, 417, 418, 418, 418, 418, 418, 418, 418, 419, 419, 419, 420, 420, 420, 420, 421, 421, 421, 421, 421, 421, 421, 422, 422, 422, 422, 423, 423, 423, 424, 424, 424, 424, 424, 424, 424, 425, 425, 425, 426, 426, 426, 426, 427, 427, 427, 427, 427, 428, 428, 428, 428, 429, 429, 429, 429, 431, 431, 431, 431, 432, 432, 432, 433, 433, 433, 434, 434, 435, 435, 435, 436, 436, 436, 436, 437, 437, 437, 437, 438, 438, 438, 438, 439, 439, 439, 439, 439, 440, 440, 440, 441, 441, 441, 441, 441, 442, 442, 442, 442, 443, 443, 443, 444, 444, 445, 445, 445, 445, 446, 446, 447, 447, 447, 447, 448, 448, 448, 448, 449, 449, 449, 449, 450, 450, 450, 450, 451, 451, 451, 451, 452, 452, 452, 452, 453, 453, 453, 453, 454, 454, 454, 454, 455, 455, 455, 455, 456, 456, 456, 456, 457, 457, 457, 457, 458, 458, 458, 458, 459, 459, 459, 460, 460, 460, 460, 461, 461, 461, 461, 462, 462, 462, 463, 463, 463, 463, 463, 464, 464, 465, 465, 465, 465, 466, 466, 466, 466, 467, 467, 467, 467, 468, 468, 469, 469, 469, 469, 469, 469, 469, 470, 470, 470, 471, 471, 471, 471] LOC -> LOC\n",
    "# [] LOC -> PER\n",
    "# [11, 25, 27, 111, 146, 182, 304, 382, 425, 433, 433, 433, 433, 433, 433, 434, 451] LOC -> ORG\n",
    "# [84, 113, 114, 137, 137, 145, 145, 182, 182, 182, 204, 204, 209, 209, 231, 231, 268, 268, 307, 323, 327, 353, 353, 387, 390, 390, 390, 395, 395, 397, 401, 401, 409, 409, 413, 413, 417, 417, 417, 432, 434, 443, 444, 446, 446, 451, 459, 464, 464, 464] LOC -> O\n",
    "\n",
    "\n",
    "\n",
    "# [58, 59, 59, 59, 98, 99, 99, 99, 105, 105, 105, 105, 109, 109, 109, 119, 119, 119, 120, 122, 127, 155, 191, 191, 191, 192, 192, 192, 195, 211, 211, 211, 214, 214, 214, 217, 217, 217, 229, 229, 229, 229, 245, 251, 252, 252, 252, 258, 258, 258, 313, 313, 313, 318, 318, 318, 318, 321, 321, 322, 322, 324, 329, 339, 339, 339, 347, 348, 355, 362, 362, 362, 362, 362, 362, 362, 364, 374, 374, 375, 375, 375, 394, 394, 394, 405, 405, 405, 407, 425, 425, 425, 425, 425, 425, 425, 440, 443, 443, 444, 448, 470, 470] ORG -> ORG\n",
    "# [444] ORG -> PER\n",
    "# [73, 96, 97, 98, 125, 256, 302, 316, 316, 344, 345, 345, 346, 347, 347, 347, 347, 350, 419] ORG -> LOC\n",
    "# [72, 72, 72, 72, 72, 72, 73, 73, 74, 75, 96, 97, 98, 98, 98, 99, 112, 112, 112, 120, 121, 121, 122, 125, 129, 133, 170, 196, 198, 198, 203, 254, 254, 254, 260, 260, 260, 260, 260, 260, 260, 266, 266, 274, 276, 276, 277, 277, 277, 279, 283, 283, 283, 289, 289, 311, 311, 322, 322, 324, 324, 324, 325, 325, 329, 334, 334, 347, 347, 347, 347, 347, 347, 348, 349, 349, 349, 355, 362, 362, 371, 371, 432, 432, 444, 444, 472, 472] ORG -> O\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ================================================== Broken ================================================\n",
    "# [1, 1, 6, 6, 9, 14, 14, 15, 15, 18, 20, 21, 21, 21, 21, 27, 27, 30, 30, 37, 38, 38, 38, 45, 46, 47, 50, 50, 52, 53, 55, 62, 63, 66, 66, 70, 72, 72, 72, 78, 80, 81, 81, 82, 82, 83, 83, 84, 84, 85, 85, 86, 87, 87, 89, 93, 94, 101, 107, 109, 110, 110, 111, 118, 125, 126, 138, 139, 142, 143, 145, 145, 146, 146, 159, 159, 160, 161, 163, 164, 165, 170, 171, 172, 173, 174, 174, 176, 177, 178, 179, 180, 181, 181, 182, 182, 187, 193, 199, 204, 204, 205, 206, 207, 208, 208, 211, 211, 212, 212, 212, 212, 212, 212, 213, 213, 214, 214, 214, 215, 215, 215, 215, 215, 215, 216, 218, 219, 219, 220, 221, 222, 223, 224, 225, 226, 227, 232, 233, 234, 237, 237, 238, 239, 244, 244, 246, 246, 247, 248, 250, 255, 255, 262, 262, 269, 272, 273, 275, 279, 280, 292, 298, 312, 312, 312, 315, 315, 325, 325, 326, 326, 330, 330, 331, 331, 333, 333, 333, 333, 334, 334, 338, 338, 338, 339, 340, 341, 342, 342, 343, 343, 351, 352, 352, 357, 359, 359, 359, 363, 363, 363, 365, 366, 366, 367, 368, 369, 370, 373, 378, 378, 379, 380, 380, 383, 383, 386, 391, 393, 397, 398, 399, 399, 400, 402, 404, 404, 417, 417, 420, 423, 425, 429, 430, 430, 430, 431, 432, 434, 436, 444, 446, 446, 447, 449, 449, 449, 451, 454, 455, 455, 455, 460, 462, 465, 465, 470, 471] PER -> PER\n",
    "# [228] PER -> LOC\n",
    "# [6, 17, 33, 65, 251, 367] PER -> ORG\n",
    "# [2, 29, 31, 35, 39, 67, 71, 77, 84, 91, 106, 110, 125, 125, 207, 213, 216, 281, 293, 317, 391, 399] PER -> O\n",
    "\n",
    "\n",
    "\n",
    "# [0, 1, 3, 3, 4, 5, 5, 5, 6, 6, 8, 9, 10, 10, 12, 13, 16, 19, 22, 22, 24, 26, 28, 30, 32, 32, 33, 33, 34, 37, 39, 40, 40, 41, 41, 42, 43, 48, 54, 56, 60, 60, 60, 60, 61, 62, 62, 62, 66, 68, 68, 68, 71, 80, 84, 84, 90, 90, 91, 91, 95, 96, 96, 96, 97, 103, 104, 104, 114, 114, 114, 115, 115, 115, 118, 123, 124, 126, 129, 129, 129, 129, 130, 130, 131, 134, 134, 134, 135, 135, 136, 136, 136, 140, 141, 147, 150, 150, 150, 150, 150, 151, 154, 154, 154, 156, 159, 159, 162, 165, 166, 168, 169, 172, 175, 182, 182, 183, 183, 184, 185, 186, 189, 193, 193, 193, 193, 196, 196, 197, 197, 197, 205, 210, 217, 230, 230, 234, 235, 235, 236, 240, 241, 241, 241, 242, 243, 246, 247, 249, 253, 256, 257, 259, 266, 267, 267, 267, 286, 288, 290, 290, 291, 294, 298, 299, 300, 300, 300, 305, 308, 309, 316, 320, 320, 323, 323, 323, 323, 328, 332, 335, 335, 336, 337, 338, 339, 341, 354, 358, 359, 360, 361, 372, 377, 377, 379, 379, 379, 379, 380, 380, 380, 380, 380, 381, 381, 381, 381, 381, 382, 382, 382, 382, 382, 383, 383, 384, 384, 384, 384, 385, 385, 385, 386, 386, 386, 387, 388, 388, 388, 388, 389, 389, 389, 389, 390, 390, 391, 391, 392, 392, 392, 392, 392, 393, 393, 393, 393, 393, 394, 394, 394, 395, 395, 395, 395, 395, 396, 396, 397, 397, 397, 397, 398, 398, 398, 399, 400, 400, 400, 401, 403, 403, 403, 403, 404, 404, 404, 404, 405, 405, 406, 406, 406, 406, 406, 406, 406, 407, 407, 408, 408, 408, 408, 409, 410, 410, 410, 410, 410, 410, 411, 411, 411, 411, 412, 412, 412, 413, 413, 413, 413, 413, 413, 413, 414, 414, 414, 414, 414, 415, 415, 415, 415, 415, 415, 415, 416, 416, 416, 416, 416, 416, 416, 417, 417, 417, 417, 417, 418, 418, 418, 418, 418, 418, 418, 419, 419, 419, 420, 420, 420, 421, 421, 421, 421, 421, 421, 421, 422, 422, 422, 422, 423, 423, 423, 424, 424, 424, 424, 424, 424, 424, 425, 425, 425, 425, 426, 426, 426, 426, 427, 427, 427, 427, 427, 428, 428, 428, 428, 429, 429, 429, 431, 431, 431, 431, 432, 432, 432, 432, 433, 433, 433, 433, 433, 433, 433, 433, 433, 434, 434, 434, 434, 435, 435, 435, 436, 436, 436, 436, 437, 437, 437, 437, 438, 438, 438, 438, 439, 439, 439, 439, 439, 440, 440, 440, 441, 441, 441, 441, 441, 442, 442, 442, 442, 443, 443, 443, 443, 444, 444, 444, 445, 445, 445, 445, 446, 446, 446, 447, 447, 447, 447, 448, 448, 448, 448, 449, 449, 449, 449, 450, 450, 450, 450, 451, 451, 451, 451, 451, 451, 452, 452, 452, 452, 453, 453, 453, 453, 454, 454, 454, 454, 455, 455, 455, 455, 456, 456, 456, 456, 457, 457, 457, 457, 458, 458, 458, 458, 459, 459, 459, 459, 460, 460, 460, 460, 461, 461, 461, 461, 462, 462, 462, 463, 463, 463, 463, 463, 464, 464, 464, 464, 464, 465, 465, 465, 465, 466, 466, 466, 466, 467, 467, 467, 467, 468, 468, 469, 469, 469, 469, 469, 469, 469, 470, 470, 470, 471, 471, 471, 471] LOC -> LOC\n",
    "# [301, 304, 319] LOC -> PER\n",
    "# [3, 25, 27, 69, 111, 146, 182, 204, 204, 303, 303, 376] LOC -> ORG\n",
    "# [11, 51, 51, 69, 79, 113, 117, 128, 131, 137, 137, 139, 139, 144, 145, 145, 151, 153, 182, 189, 202, 209, 209, 213, 231, 231, 268, 268, 306, 307, 307, 307, 327, 353, 353, 390, 390, 401, 401, 409, 409, 420, 429, 446] LOC -> O\n",
    "\n",
    "\n",
    "\n",
    "# [58, 59, 59, 59, 74, 96, 97, 99, 99, 99, 105, 105, 105, 105, 109, 109, 109, 112, 112, 112, 119, 119, 119, 120, 127, 191, 191, 191, 195, 211, 211, 211, 214, 214, 214, 217, 217, 217, 229, 229, 229, 229, 245, 251, 252, 252, 252, 254, 258, 258, 258, 260, 260, 260, 260, 260, 313, 313, 313, 318, 318, 318, 318, 321, 321, 322, 324, 324, 324, 329, 339, 339, 339, 345, 347, 347, 347, 347, 347, 347, 347, 347, 347, 349, 349, 355, 362, 362, 362, 362, 362, 362, 362, 362, 362, 374, 374, 375, 375, 375, 394, 394, 394, 405, 405, 405, 407, 419, 425, 425, 425, 425, 425, 425, 425, 432, 432, 440, 443, 443, 444, 444, 444, 444, 448, 470, 470, 472, 472] ORG -> ORG\n",
    "# [364] ORG -> PER\n",
    "# [72, 72, 73, 99, 122, 125, 125, 256, 302, 316, 316, 344, 345, 350] ORG -> LOC\n",
    "# [72, 72, 72, 72, 73, 73, 75, 96, 97, 98, 98, 98, 98, 98, 120, 121, 121, 122, 129, 133, 155, 170, 192, 192, 192, 196, 198, 198, 203, 254, 254, 260, 260, 266, 266, 274, 276, 276, 277, 277, 277, 279, 283, 283, 283, 289, 289, 311, 311, 322, 322, 322, 324, 325, 325, 329, 334, 334, 346, 347, 347, 348, 348, 349, 355, 371, 371] ORG -> O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([72, 72, 72, 72, 73, 73, 75, 96, 97, 98, 98, 98, 98, 98, 120, 121, 121, 122, 129, 133, 155, 170, 192, 192, 192, 196, 198, 198, 203, 254, 254, 260, 260, 266, 266, 274, 276, 276, 277, 277, 277, 279, 283, 283, 283, 289, 289, 311, 311, 322, 322, 322, 324, 325, 325, 329, 334, 334, 346, 347, 347, 348, 348, 349, 355, 371, 371])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.to_csv(\"baseline_model_broken_entity_not_recognized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar_df = combined_df.loc[orig_asr_similar['Sample #'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_asr_similar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "sample = []\n",
    "for sample_no, model_tag, asr_tag in zip(orig_asr_similar_df.index, orig_asr_similar_df['original_tags'].values.tolist(), orig_asr_similar_df['asr_tags'].values.tolist()):\n",
    "    x_tag = model_tag.split(\",\")\n",
    "    y_tag = asr_tag.split(\",\")\n",
    "    for x,y in zip(x_tag, y_tag):\n",
    "        if x == \"ORG\" and x != y:\n",
    "            sample.append(sample_no)\n",
    "print(sample)\n",
    "# [2, 17, 29, 31, 33, 35, 39, 65, 67, 71, 77, 91, 106, 228, 251, 281, 293, 317] --- 11.464968152866243 --- 18 PER broken\n",
    "# [11, 25, 27, 51, 51, 69, 69, 79, 111, 113, 117, 128, 137, 137, 139, 139, 144, 145, 145, 146, 153, 202, 204, 204, 209, 209, 213, 231, 231, 268, 268, 301, 303, 303, 304, 306, 307, 307, 307, 319, 327, 353, 353] --- 18.867924528301888 --- 30 LOC broken\n",
    "# [72, 72, 72, 72, 72, 72, 73, 73, 73, 75, 98, 98, 98, 98, 98, 121, 121, 122, 122, 129, 133, 155, 170, 192, 192, 192, 196, 198, 198, 203, 256, 266, 266, 274, 276, 276, 277, 277, 277, 279, 283, 283, 283, 289, 289, 302, 311, 311, 316, 316, 325, 325, 334, 334, 344, 346, 348, 348, 350, 364, 371, 371] --- 45.83333333333333 --- 33 --- ORG broken\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [2, 17, 31, 34, 39, 65, 65, 67, 67, 71, 77, 91, 94, 107, 125, 125, 125, 134, 137, 175, 207, 207, 228, 251, 269, 281, 293, 356, 367, 367] --- 14.906832298136646 --- 24 PER\n",
    "# [100, 111, 113, 127, 145, 145, 146, 152, 152, 182, 182, 182, 182, 194, 194, 200, 201, 201, 231, 231, 244, 251, 268, 268, 268, 314, 314, 314, 353, 353] --- 10.897435897435898 --- 17 LOC\n",
    "# [10, 10, 21, 37, 72, 72, 72, 72, 72, 72, 73, 73, 73, 74, 74, 74, 74, 74, 74, 75, 75, 89, 89, 89, 89, 96, 96, 97, 97, 102, 102, 102, 112, 112, 112, 121, 121, 125, 125, 129, 129, 129, 129, 132, 133, 134, 134, 134, 135, 135, 136, 136, 137, 137, 148, 156, 158, 158, 158, 165, 170, 178, 196, 198, 198, 198, 199, 203, 203, 203, 204, 204, 204, 204, 209, 209, 213, 254, 254, 254, 255, 255, 255, 256, 260, 260, 260, 260, 260, 260, 260, 261, 262, 263, 264, 265, 266, 266, 269, 270, 271, 274, 276, 276, 277, 277, 277, 278, 279, 283, 283, 283, 284, 285, 289, 289, 297, 302, 308, 311, 311, 314, 315, 316, 316, 325, 325, 327, 334, 334, 334, 334, 345, 345, 346, 349, 349, 349, 350, 371, 371] --- 62.28070175438597 --- 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_df.loc[[72, 72, 72, 72, 72, 72, 73, 73, 73, 75, 98, 98, 98, 98, 98, 121, 121, 122, 122, 129, 133, 155, 170, 192, 192, 192, 196, 198, 198, 203, 256, 266, 266, 274, 276, 276, 277, 277, 277, 279, 283, 283, 283, 289, 289, 302, 311, 311, 316, 316, 325, 325, 334, 334, 344, 346, 348, 348, 350, 364, 371, 371]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = []\n",
    "for sample_no, model_tag, asr_tag in zip(df.index, df['original_tags'].values.tolist(), df['asr_tags'].values.tolist()):\n",
    "    x_tag = model_tag.split(\",\")\n",
    "    y_tag = asr_tag.split(\",\")\n",
    "    for x,y in zip(x_tag, y_tag):\n",
    "        if x == \"ORG\" and y == \"LOC\":\n",
    "            sample.append(sample_no)\n",
    "print(sample)\n",
    "# [17, 33, 65, 251] PER -> ORG\n",
    "# [228] PER -> LOC\n",
    "# [2, 29, 31, 35, 39, 67, 71, 77, 91, 106, 281, 293, 317] PER -> O\n",
    "\n",
    "\n",
    "# [301, 304, 319] LOC -> PER\n",
    "# [25, 27, 69, 69, 111, 146, 204, 204, 204, 204, 303, 303, 303, 303] LOC -> ORG\n",
    "# [11, 51, 51, 51, 51, 69, 69, 79, 113, 117, 128, 137, 137, 137, 137, 139, 139, 139, 139, 144, 145, 145, 145, 145, 153, 202, 209, 209, 209, 209, 213, 231, 231, 231, 231, 268, 268, 268, 268, 306, 307, 307, 307, 307, 307, 307, 307, 307, 307, 327, 353, 353, 353, 353] LOC -> O\n",
    "\n",
    "\n",
    "# [364] ORG -> PER\n",
    "# [72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 122, 122, 256, 302, 316, 316, 316, 316, 344, 350] ORG -> LOC\n",
    "# [72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 75, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 121, 121, 121, 121, 122, 122, 129, 133, 155, 170, 192, 192, 192, 192, 192, 192, 192, 192, 192, 196, 198, 198, 198, 198, 203, 266, 266, 266, 266, 274, 276, 276, 276, 276, 277, 277, 277, 277, 277, 277, 277, 277, 277, 279, 283, 283, 283, 283, 283, 283, 283, 283, 283, 289, 289, 289, 289, 311, 311, 311, 311, 325, 325, 325, 325, 334, 334, 334, 334, 346, 348, 348, 348, 348, 371, 371, 371, 371] ORG -> O\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [39, 65, 65, 65, 65, 94, 107, 251, 367, 367, 367, 367] PER -> ORG\n",
    "# [17, 31, 34, 77, 269] PER -> LOC\n",
    "# [2, 67, 67, 67, 67, 71, 91, 125, 125, 125, 125, 125, 125, 125, 125, 125, 134, 137, 175, 207, 207, 207, 207, 228, 281, 293, 356] PER -> O\n",
    "\n",
    "\n",
    "# [244] LOC -> PER\n",
    "# [111, 127, 146, 182, 182, 182, 182, 251] LOC -> ORG\n",
    "# [100, 113, 145, 145, 145, 145, 152, 152, 152, 152, 182, 182, 182, 182, 182, 182, 182, 182, 182, 182, 182, 182, 194, 194, 194, 194, 200, 201, 201, 201, 201, 231, 231, 231, 231, 268, 268, 268, 268, 268, 268, 268, 268, 268, 314, 314, 314, 314, 314, 314, 314, 314, 314, 353, 353, 353, 353] LOC -> O\n",
    "\n",
    "\n",
    "# [] ORG -> PER\n",
    "# [10, 10, 37, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 96, 96, 97, 97, 125, 125, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 129, 134, 134, 134, 134, 134, 134, 134, 134, 134, 136, 136, 136, 136, 156, 199, 213, 256, 302, 316, 316, 316, 316, 345, 345, 345, 345, 346, 350] ORG -> LOC\n",
    "# [10, 10, 21, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 96, 96, 97, 97, 102, 102, 102, 102, 102, 102, 102, 102, 102, 112, 112, 112, 112, 112, 112, 112, 112, 112, 121, 121, 121, 121, 125, 125, 129, 129, 129, 129, 132, 133, 135, 135, 135, 135, 137, 137, 137, 137, 148, 158, 158, 158, 158, 158, 158, 158, 158, 158, 165, 170, 178, 196, 198, 198, 198, 198, 198, 198, 198, 198, 198, 203, 203, 203, 203, 203, 203, 203, 203, 203, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 204, 209, 209, 209, 209, 254, 254, 254, 254, 254, 254, 254, 254, 254, 255, 255, 255, 255, 255, 255, 255, 255, 255, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 261, 262, 263, 264, 265, 266, 266, 266, 266, 269, 270, 271, 274, 276, 276, 276, 276, 277, 277, 277, 277, 277, 277, 277, 277, 277, 278, 279, 283, 283, 283, 283, 283, 283, 283, 283, 283, 284, 285, 289, 289, 289, 289, 297, 308, 311, 311, 311, 311, 314, 315, 325, 325, 325, 325, 327, 334, 334, 334, 334, 334, 334, 334, 334, 334, 334, 334, 334, 334, 334, 334, 334, 349, 349, 349, 349, 349, 349, 349, 349, 349, 371, 371, 371, 371] ORG -> O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 75, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 121, 121, 121, 121, 122, 122, 129, 133, 155, 170, 192, 192, 192, 192, 192, 192, 192, 192, 192, 196, 198, 198, 198, 198, 203, 266, 266, 266, 266, 274, 276, 276, 276, 276, 277, 277, 277, 277, 277, 277, 277, 277, 277, 279, 283, 283, 283, 283, 283, 283, 283, 283, 283, 289, 289, 289, 289, 311, 311, 311, 311, 325, 325, 325, 325, 334, 334, 334, 334, 346, 348, 348, 348, 348, 371, 371, 371, 371])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[[39, 65, 65, 65, 65, 94, 107, 251, 367, 367, 367, 367]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_nofound.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "# [83.43949044585987, 5.095541401273886, 11.464968152866243, 0.0] PER broken\n",
    "# [131, 8, 18, 0] PER broken count\n",
    "\n",
    "\n",
    "# [77.9874213836478, 3.1446540880503147, 18.867924528301888, 0.0] LOC broken\n",
    "# [124, 5, 30, 0] LOC broken count\n",
    "\n",
    "\n",
    "# [37.5, 16.666666666666664, 45.83333333333333, 0.0] ORG broken\n",
    "# [124, 5, 30, 0] ORG broken count\n",
    "\n",
    "\n",
    "# [198.94, 24.89, 76.15] total broken\n",
    "# [66.31, 8.29, 25.38]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# [80.74534161490683, 4.3478260869565215, 14.906832298136646, 0.0] PER\n",
    "# [130, 7, 24, 0] PER\n",
    "\n",
    "\n",
    "# [84.61538461538461, 4.487179487179487, 10.897435897435898, 0.0] LOC\n",
    "# [132, 7, 17, 0] LOC\n",
    "\n",
    "\n",
    "# [21.929824561403507, 15.789473684210526, 62.28070175438597, 0.0] ORG\n",
    "# [25, 18, 71, 0] ORG\n",
    "\n",
    "\n",
    "# [187.27, 24.6, 88.07]\n",
    "# [62.42, 8.20, 29.35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [131, 8, 18, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.concat([orig_asr_found, orig_asr_similar], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.loc[error_df['Sample #'].values.tolist()]\n",
    "# [153,154,13,47,1,52,78,20,30,150]\n",
    "# [6, 84, 110, 125, 207, 213, 216, 367, 2, 17, 29, 31, 33, 35, 39, 65, 67, 71, 77, 91, 106, 228, 251, 281, 293, 317]\n",
    "#[72, 84, 110, 208, 213, 215, 367, 2, 17, 31, 35, 39, 65, 67, 71, 77, 78, 91, 94, 106, 125, 177, 199, 207, 223, 224, 225, 228, 251, 269, 272, 279, 280, 281, 293, 317]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = combined_df.loc[error_df['Sample #'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for model_tag, asr_tag in zip(new_df['original_tags'].values.tolist(), new_df['asr_tags'].values.tolist()):\n",
    "    x_tag = model_tag.split(\",\")\n",
    "    y_tag = asr_tag.split(\",\")\n",
    "    total = total + len(x_tag)\n",
    "    for x,y in zip(x_tag, y_tag):\n",
    "        if x == y and x != \"O\" and y != \"O\":\n",
    "            count = count + 1\n",
    "print(count)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23 (broken entity with simulation) / 472 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_analysis(sample_df, combined_df):\n",
    "    ind = np.array(sample_df['Sample #'].values.tolist())\n",
    "    df = combined_df.loc[ind]\n",
    "    df.insert(2,'Original',sample_df['Original'].values.tolist())\n",
    "    df.insert(5,'ASR',sample_df['ASR'].values.tolist())\n",
    "    df.drop(['original_tags', 'asr_tags'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern = pattern_analysis(orig_asr_similar, combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling(df):\n",
    "    i = 0\n",
    "    equal_length_samples = []\n",
    "    variable_length_samples = []\n",
    "    for sample, original, asr in zip(df.index, \n",
    "                                     df['Original'],\n",
    "                                     df['ASR']):\n",
    "        if len(original) == len(asr):\n",
    "            equal_length_samples.append(sample)\n",
    "        else:\n",
    "            variable_length_samples.append(sample)\n",
    "    equal_length_samples.sort()\n",
    "    variable_length_samples.sort()\n",
    "    equal_length_samples_df = df.loc[equal_length_samples]\n",
    "    variable_length_samples_df = df.loc[variable_length_samples]\n",
    "    return equal_length_samples_df, variable_length_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df, variable_length_words_samples_df = error_sampling(error_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(variable_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variable_length_words_samples_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_words_simulation(sampled_df):\n",
    "    simulated_asr = []\n",
    "    for sample, original_sentence, asr_sentence, original, asr in zip(sampled_df.index,\n",
    "                                     sampled_df['original_sentence'],\n",
    "                                     sampled_df['asr_sentence'],\n",
    "                                     sampled_df['Original'],\n",
    "                                     sampled_df['ASR']):\n",
    "\n",
    "        for x,y in zip(original, asr):\n",
    "            #original_words.append(x)\n",
    "            #asr_words.append(y)\n",
    "            if y in asr_sentence:\n",
    "                asr_sentence = asr_sentence.replace(y, x)\n",
    "            \n",
    "        simulated_asr.append((sample, asr_sentence))\n",
    "    simulated_asr_df = pd.DataFrame(simulated_asr)\n",
    "    return simulated_asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_words_simulation(df):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "            df.index,\n",
    "            df['original_sentence'].values.tolist(),\n",
    "            df['asr_sentence'].values.tolist(),\n",
    "            df['Original'].values.tolist(),\n",
    "            df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        o_label = original_label[original_tag_ind]\n",
    "        for lab in original_tag:\n",
    "            for asr_lab in asr_tag:\n",
    "                local_error = (1 - (Levenshtein.distance(lab, asr_lab) / max(len(lab), len(asr_lab)))) * 100\n",
    "                if local_error >= 50.0:\n",
    "                    asr_sentence = asr_sentence.replace(asr_lab, lab)\n",
    "        check.append((sample, asr_sentence))\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df(asr_df, simulated_df):\n",
    "    asr_df.loc[simulated_df[0].values.tolist(), 'Sentence'] = simulated_df[1].values.tolist()\n",
    "    return asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = equal_words_simulation(equal_length_words_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.loc[simulated_asr_df[0].values.tolist(), 'Sentence'] = test_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulated_asr_df = variable_words_simulation(variable_length_words_samples_df)\n",
    "#simulated_asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = test_df[test_df['token'] == \"[CLS]\" ].index\n",
    "test_df.drop(indexNames, inplace=True)\n",
    "indexNames = test_df[test_df['token'] == \"[SEP]\" ].index\n",
    "test_df.drop(indexNames, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = prepare_model_output(test_df, new_df)\n",
    "test_df = prepare_model_output(test_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = test_df.groupby(\"sentence_no\")\n",
    "test = pd.DataFrame({\"model_tag\": g_test.apply(lambda sdf: sdf.labels.values.tolist()),\n",
    "                       \"asr_tag\": g_test.apply(lambda sdf: sdf.label_asr.values.tolist())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['asr_sentence_no'] = test.index\n",
    "test[[\"asr_sentence_no\"]] = test[[\"asr_sentence_no\"]].apply(pd.to_numeric)\n",
    "test.sort_values('asr_sentence_no', inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" , accuracy_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "print(\"F1 Score: \",f1_score(test['model_tag'].values.tolist(), test['asr_tag'].values.tolist()))\n",
    "#statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])\n",
    "#0.7758389261744967 without punctuation\n",
    "#0.676056338028169 with punctuation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"ORG\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]\n",
    "plt.bar(['Correctly Identified', 'Identified with missing entities', 'Similar tag but not identified', 'No Tag identification'], data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_similar.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_simulated_df = combined_df.loc[orig_asr_similar['Sample #'].values.tolist(),['original_sentence','original_tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_simulated_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_test = df.groupby(\"Sentence #\")\n",
    "x = pd.DataFrame({\"Sentence\": g_test.apply(lambda sdf: \" \".join(sdf.Word)),\n",
    "                       \"Tag\": g_test.apply(lambda sdf: \",\".join(sdf.Tag))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no = list(range(0, len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.index = sentence_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.loc[context_simulated_df.index.tolist(), 'Sentence'] = context_simulated_df['original_sentence'].values.tolist()\n",
    "x.loc[context_simulated_df.index.tolist(), 'Tag'] = context_simulated_df['original_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_simulated_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.loc[context_simulated_df.index.tolist(), 'Sentence'] = context_simulated_df['original_sentence'].values.tolist()\n",
    "asr_df.loc[context_simulated_df.index.tolist(), 'Tag'] = context_simulated_df['original_tags'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = model_test(asr_df['Sentence'].values.tolist(), tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no = 0\n",
    "dataset=[]\n",
    "for sentences, tags in zip(x['Sentence'].values.tolist(), x['Tag'].values.tolist()):\n",
    "    sentence=sentences.split(\" \")\n",
    "    tag = tags.split(\",\")\n",
    "    for word, label in zip(sentence, tag):\n",
    "        dataset.append((sentence_no, word, label))\n",
    "    sentence_no = sentence_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(dataset, columns=['Sentence #', 'Word', 'Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = prepare_model_output(test_df, new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics(test_df, ['PER', 'ORG', 'LOC', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_csv('unprocessed_sampled_original.csv')\n",
    "original.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "original = original[:7851]\n",
    "g_original = original.groupby(\"Sentence #\")\n",
    "original_df = pd.DataFrame({'Sentence': g_original.apply(lambda sdf: \" \".join(map(str,sdf.Word))),\n",
    "                      'Tag': g_original.apply(lambda sdf: \",\".join(sdf.Tag))})\n",
    "original_df.reset_index(inplace=True)\n",
    "combined_df = pd.DataFrame({\"original_sentence\": original_df['Sentence'],\n",
    "                           \"original_tags\": original_df['Tag'], \n",
    "                           \"asr_sentence\": asr_df['Sentence'],\n",
    "                           \"asr_tags\": asr_df['Tag']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df, combined_df = prepare_data_for_analysis(test_df, 'unprocessed_sampled_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.DataFrame(pattern_finding(\"PER\", combined_df), columns=['Sample #', 'Original', 'ASR', 'Lavenstein','Lavenstein Mean', 'Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found_complete = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] == 100.0)]\n",
    "orig_asr_found_complete_per = (len(orig_asr_found_complete) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_complete_per)\n",
    "orig_asr_found_complete.head()\n",
    "print(len(orig_asr_found_complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_found = analysis_df[(analysis_df['Flag'] == True) & (analysis_df['Lavenstein Mean'] < 100.0) & (analysis_df['Lavenstein Mean'] >= 0.0)]\n",
    "orig_asr_found_per = (len(orig_asr_found) / len(analysis_df)) * 100\n",
    "print(orig_asr_found_per)\n",
    "print(len(orig_asr_found))\n",
    "orig_asr_found.head()\n",
    "#40.88050314465409\n",
    "#65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 100.0) & (analysis_df['Lavenstein Mean'] > 0.0)]\n",
    "orig_asr_similar_per = (len(orig_asr_similar) / len(analysis_df)) * 100\n",
    "print(orig_asr_similar_per)\n",
    "orig_asr_similar.head()\n",
    "print(len(orig_asr_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_asr_similar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_asr_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_asr_nofound = analysis_df[(analysis_df['Flag'] == False) & (analysis_df['Lavenstein Mean'] <= 0.0)]\n",
    "orig_asr_nofound_per = (len(orig_asr_nofound) / len(analysis_df))*100\n",
    "print(orig_asr_nofound_per)\n",
    "orig_asr_nofound.head()\n",
    "print(len(orig_asr_nofound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[orig_asr_found_complete_per, orig_asr_found_per, orig_asr_similar_per, orig_asr_nofound_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(orig_asr_found_complete), len(orig_asr_found), len(orig_asr_similar), len(orig_asr_nofound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_length_words_samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_context(df, n_grams):\n",
    "    check = []\n",
    "    for sample, original_sentence, asr_sentence, original_tag, asr_tag in zip(\n",
    "                df.index,\n",
    "                df['original_sentence'].values.tolist(),\n",
    "                df['asr_sentence'].values.tolist(),\n",
    "                df['Original'].values.tolist(),\n",
    "                df['ASR'].values.tolist()):\n",
    "\n",
    "        original_label = np.array(original_sentence.split())\n",
    "        asr_label = np.array(asr_sentence.split())\n",
    "        original_tag_ind = [index for index, element in enumerate(original_label) if original_label[index] in original_tag]\n",
    "        asr_tag_ind = [index for index, element in enumerate(asr_label) if asr_label[index] in asr_tag]\n",
    "        original_bigrams = []\n",
    "        asr_bigrams = []\n",
    "        for l in original_tag_ind:\n",
    "            if l <= (len(original_label)-1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams+1, 1):\n",
    "                    if l+c >= 0:\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l+c < len(original_label):\n",
    "                        data = data + original_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                original_bigrams.append(data)\n",
    "        for l in asr_tag_ind:\n",
    "            if l <= (len(asr_label) - 1) - n_grams:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, n_grams + 1, 1):\n",
    "                    if l + c >= 0:\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "            else:\n",
    "                data = \"\"\n",
    "                for c in range(-n_grams, 1, 1):\n",
    "                    if l + c < len(asr_label):\n",
    "                        data = data + asr_label[l + c] + \" \"\n",
    "                    else:\n",
    "                        continue\n",
    "                asr_bigrams.append(data)\n",
    "        \n",
    "        check.append((sample, original_bigrams[0], original_sentence, original_tag, asr_bigrams[0], asr_sentence, asr_tag))\n",
    "    context = pd.DataFrame(check)\n",
    "    context.columns = ['Sample #', 'Original N-Grams', \"original_sentence\", \"Original\", \"ASR N-Grams\", \"asr_sentence\", \"ASR\"]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_sampling3(context):\n",
    "    check = []\n",
    "    for sample, original_ngrams, original_sentence, asr_ngrams, asr_sentence, original_tag, asr_tag in zip(\n",
    "            context['Sample #'].values.tolist(),\n",
    "            context['Original N-Grams'].values.tolist(),\n",
    "            context['original_sentence'].values.tolist(),\n",
    "            context['ASR N-Grams'].values.tolist(),\n",
    "            context['asr_sentence'].values.tolist(),\n",
    "            context['Original'].values.tolist(),\n",
    "            context['ASR'].values.tolist()):\n",
    "        \n",
    "        original_ngrams = np.array(original_ngrams.split(\" \"))\n",
    "        asr_ngrams = np.array(asr_ngrams.split(\" \"))\n",
    "        \n",
    "        local_errors = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for _original in original_tag:\n",
    "            if _original in asr_tag:\n",
    "                if len(asr_ngrams) < len(asr_tag):\n",
    "                    continue\n",
    "                \n",
    "                print(asr_sentence)\n",
    "                asr_sentence = asr_sentence.replace(\"\".join(asr_ngrams[i].rstrip()), \"\".join(original_ngrams[i].rstrip()))\n",
    "                print(asr_sentence)\n",
    "                #print(check)\n",
    "                i = i + 1\n",
    "                j = j + 1\n",
    "            else:\n",
    "                j = j + 1\n",
    "        check.append((sample, asr_sentence))\n",
    "        print(\"---------------\")\n",
    "    new_asr = pd.DataFrame(check)\n",
    "    return new_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = finding_context(equal_length_words_samples_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_asr_df = error_sampling3(context)\n",
    "simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context = finding_context(variable_length_words_samples_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulated_asr_df = error_sampling3(context)\n",
    "#simulated_asr_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asr_df = update_df(asr_df, simulated_asr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"and though the famous family of aldus restored its technical excellence , rejecting battered letters ,\",\"most of caxton ' s zone types of an earlier character\", \"are the leaders in this luckless change , though our own baskerville , who was at work some years before them , went much on the same lines\",\n",
    "       \"now come into general use that are obviously a great improvement on the ordinary \\\" modern style \\\" and use in england , which is in fact the bodoni type\" , \"on the top of the jail , continues neild , arawatch - house and a century - box , where two or more guards , with dogs and firearms ,\" ,\n",
    "       \"these courts were extended to centuries later to several large provincial towns , and all were in full activity when neild road ,\" , \"he had been in the employ of a corn - chandler at islington , and went into london with his master ' s cart and horse .\",\n",
    "       \"shameful malpractices of bambridge ,\" , \"if they happened to be in funds - - among whom was the marquis of slego in 1811\", \n",
    "       \"mister . neild , a second howard ,\", \"again the 22 charles ii . c20 order the jailer to keep felons and debtors \\\" separate and apart from one another ,\",\n",
    "       \"prisoners were crowded together in the jail , contrary to the requirements of the for george the 4th .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"have now come into general use and are obviously a great improvement on the ordinary \\\" modern style \\\" in use in england , which is in fact the bodoni type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = model_test([test], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
